{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:59:22.787551Z",
     "start_time": "2024-12-16T08:59:22.783931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import AutoTokenizer\n"
   ],
   "id": "baf14d9e89604217",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:50:01.979166Z",
     "start_time": "2024-12-16T08:50:01.916824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tips：加入 tokenizer \n",
    "# tokenizer 目录路径\n",
    "local_tokenizer_dir = \"./tokenizer_files/\"  # 替换为你的实际路径\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_tokenizer_dir,\n",
    "    trust_remote_code=True,  # 如果使用的分词器有自定义代码，需要启用此选项\n",
    "    truncation_side='right', # 设置分词器的截断侧\n",
    "    padding_side='right'     # 设置分词器的填充侧\n",
    ")"
   ],
   "id": "563978ee2481169d",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T08:50:02.523104Z",
     "start_time": "2024-12-16T08:50:02.518450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tips：初始函数设置\n",
    "# 参数设置\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 1536\n",
    "num_heads = 8\n",
    "num_decoder_layers = 2\n",
    "dim_feedforward = 256\n",
    "max_seq_length = 5\n",
    "dropout = 0.1\n",
    "batch_size = 2\n"
   ],
   "id": "8f977da972ea1792",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在多头注意力中，每个注意力头的计算遵循**Scaled Dot-Product Attention**公式：\n",
    "\n",
    "#### **2.1. 单头注意力公式**\n",
    "\n",
    "对于每个头，计算方式为：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "- $Q$: 查询向量 (query)。\n",
    "- $K$: 键向量 (key)。\n",
    "- $V$: 值向量 (value)。\n",
    "- $d_k$: 每个头的键/查询向量维度。\n",
    "#### **2.2. 多头注意力公式**\n",
    "\n",
    "多头注意力通过多个头并行计算注意力，然后将结果合并：\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "- 每个注意力头的计算：\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "$W_i^Q, W_i^K, W_i^V$ 是每个头的权重矩阵。\n",
    "$$\n",
    "\\text{FFN}(x)=max(0, xW_1+b_1)W_2+b_2\n",
    "$$"
   ],
   "id": "9d34c9aff65d1733"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "828c3019ca77c41b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![formula](../decoder_img/formula_multi-head%20attention.png)",
   "id": "3040e90b321ac5f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2837cf4a7f333c23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![encoder-decoder](../decoder_img/encoder-decoder.png)\n",
    "![sdqa](../decoder_img/sdpa.png)\n",
    "![multi-head-attention](../decoder_img/multi-head%20attention.png)"
   ],
   "id": "67f1fc75d93df0c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cfe64e12799b2dbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "197cabf3b929509b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "240e31e07d504e9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:51:47.236506Z",
     "start_time": "2024-12-14T02:51:47.229339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tips：多头自注意力机制\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # 定义线性变换层\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "    def split_head(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, head_dim).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # 线性变换\n",
    "        q = self.q_linear(q)  # (batch_size, seq_len_q, hidden_size)  对 emb 的值或者维度做了改变\n",
    "        k = self.k_linear(k)  # (batch_size, seq_len_k, hidden_size)\n",
    "        v = self.v_linear(v)  # (batch_size, seq_len_v, hidden_size)\n",
    "        print(f\"q.size: {q.size()}\")\n",
    "        print(f\"k.size: {k.size()}\")\n",
    "        print(f\"v.size: {v.size()}\")\n",
    "        \n",
    "        # 分割头\n",
    "        q = self.split_head(q, batch_size)  # (batch_size, num_heads, seq_len_q, head_dim)\n",
    "        k = self.split_head(k, batch_size)  # (batch_size, num_heads, seq_len_k, head_dim)\n",
    "        v = self.split_head(v, batch_size)  # (batch_size, num_heads, seq_len_v, head_dim)\n",
    "        print(f\"new_q.size: {q.size()}\")\n",
    "        print(f\"new_k.size: {k.size()}\")\n",
    "        print(f\"new_v.size: {v.size()}\")\n",
    "    \n",
    "        # 每头独立计算\n",
    "        ## 多头计算注意力分数 QK^T\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        if mask is not None:\n",
    "            # scores 张量中对应于 mask 中为0的位置（即不应该被关注的位置），将其值设置为 -1e9\n",
    "            # 这样-1e9在经过softmax以后得到的概率接近于零\n",
    "            # 实现了 mask 中掩码为0的位置是没有score分数的\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        ## 减少计算使用过设置为负无穷大，然后负无穷大的对应函数值为0，0不参与计算实现的\n",
    "        ## softmax 作用与最后一维，负无穷大对应的是 0，对应到 v 就不会有最后向量\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        ## 应用注意力权重到值向量\n",
    "        output = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, head_dim)\n",
    "\n",
    "        # 合并头\n",
    "        ######## 多头是如何实现的？？？？\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)  # (batch_size, seq_len_q, hidden_size)\n",
    "        # (batch_size, num_heads, seq_len, head_dim) 变化 transpose(1, 2)\n",
    "        # (batch_size, seq_len, num_heads, head_dim)\n",
    "        # view 之前用 contiguous() 是为了确保张量在内存中的布局是连续的\n",
    "        # view 进行重塑，将 num_heads 和 head_dim 合并在一起\n",
    "        \n",
    "        # 最后一个线性变换\n",
    "        output = self.o_linear(output)  # (batch_size, seq_len_q, hidden_size)\n",
    "\n",
    "        return output, attention_weights"
   ],
   "id": "e36ce09a6d3d1149",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![FFN](../decoder_img/FFN.png)",
   "id": "756d2b41394f38ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:51:49.205858Z",
     "start_time": "2024-12-14T02:51:49.201848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tips：FNN\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)  # 从 d_model 映射到 d_ff\n",
    "        self.fc2 = nn.Linear(d_ff, d_model) # 再从 d_ff 回到 d_model\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n"
   ],
   "id": "2ca36ed3738b96a0",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![pe](../decoder_img/position%20embedding.png)",
   "id": "a4b21fe4c71115f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:51:50.537326Z",
     "start_time": "2024-12-14T02:51:50.531303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tips：位置编码\n",
    "# 继承了PyTorch的 nn.Module 类，这意味着它可以像其他PyTorch模块一样被使用\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # 构造函数接收两个参数 d_model：嵌入向量大小，max_len：最长序列长度\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model) # (max_len, d_model) 存储位置编码\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # ①\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 偶数维赋值  # ②\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 奇数维赋值  # ②\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # [max_len, 1, d_model]\n",
    "        self.register_buffer('pe', pe) # 注册为一个缓冲区，被保存在模型的字典中，不会参与梯度更新\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # 调整位置编码大小：根据输入 x 的实际序列长度切片位置编码矩阵，确保只添加必要的部分。\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, vocab_size):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # 自注意力层（Causal-Attention）\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)     \n",
    "        # 编码器-解码器注意力层（Cross-Attention）\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # 前馈网络（Feed-Forward）\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        \n",
    "        # LayerNorm 层（Layer Normalization）\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout 层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # 将输入索引转换为嵌入向量，并调整尺度\n",
    "       \n",
    "        # 自注意力：输入为解码器自身的输入（x），q,k,v全部都是x \n",
    "        attn_output, block1 = self.self_attn(x, x, x, tgt_mask)  # [bs, seq_len, emb] [bs, 1, seq_len, seq_len]\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # 残差连接和 LayerNorm\n",
    "        \n",
    "        # 交叉注意力：输入为编码器输出（enc_output）和解码器输入（x）\n",
    "        # q：decoder's output. k + v: encoder's output \n",
    "        attn_output, block2 = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))  # 残差连接和 LayerNorm\n",
    "        \n",
    "        # 前馈网络（Feed-Forward）\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))  # 残差连接和 LayerNorm\n",
    "        \n",
    "        return x, block1, block2"
   ],
   "id": "d56752bc82caa324",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:51:51.700675Z",
     "start_time": "2024-12-14T02:51:51.694768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tips：Decoder 层\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout, vocab_size, max_len=5000):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # 建立模型的 emb 层  创建一个形状为 (vocab_size, d_model) 的可训练参数矩阵\n",
    "        # 初始化：嵌入矩阵的权重通常会在初始化时随机分配，或者从预训练模型中加载。\n",
    "        # 训练：在训练过程中，嵌入矩阵的权重会通过反向传播进行更新，以优化模型性能。\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 位置编码\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
    "        \n",
    "        # 解码器层列表\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout, vocab_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attention_weights = {}\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 添加嵌入和位置编码 \n",
    "        ## 嵌入的扩大：乘以嵌入维度的平方根 嵌入向量的初始值通常是从一个较小的标准差分布中随机抽取的，比如标准正态分布或均匀分布。如果直接将这些小数值输入到后续的线性变换、激活函数等操作中，可能会导致信号逐渐衰减，特别是当网络层数较多时。通过乘以嵌入维度的平方根，可以放大这些初始值，使得它们在整个网络中的传播更为稳定。\n",
    "        # 在 Transformer 模型中，位置编码（positional encoding）会直接加到词嵌入（token embeddings）上。位置编码的设计通常是基于正弦和余弦函数，其幅度大致为 1。如果不调整词嵌入的规模，那么位置编码的影响可能会过大或过小，从而破坏了两者之间的相对比例。乘以 sqrt(d_model) 可以使词嵌入的均方根（RMS, Root Mean Square）与位置编码相近，维持两者在一个相似的数量级上，避免一方压倒另一方。\n",
    "        # 这种做法源自 Vaswani 等人在他们的论文《Attention Is All You Need》中提出的建议。他们指出，这样的缩放有助于保持模型各部分的输出具有相似的方差，进而促进更有效的学习。\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        ## final output 用来进行输出， 输出和attention weight 是不一样的\n",
    "        ## block1 和 block2 是自注意力和交叉注意力 的参数，所以是不是模型内部的参数指的就是 block1 和 block2 的参数\n",
    "        ## 遍历 self.dec_layers 列表中的每一层\n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "            x, block1, block2 = dec_layer(x, enc_output, src_mask, tgt_mask)  # [bs, seq_len, emb] [bs, seq_len, emb], [bs, heads, 1, seq_len], [bs, 1, seq_len, seq_len]\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "        \n",
    "        # 最后通过全连接层映射到词汇表大小\n",
    "        final_output = self.fc_out(x)  # (batch_size, target_seq_len, vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights"
   ],
   "id": "bbc35dc2c34025f0",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:51:52.363557Z",
     "start_time": "2024-12-14T02:51:52.356089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tips：trans 整合\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_decoder_layers, dim_feedforward, max_seq_length, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # 加载分词器\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"./tokenizer_files/\",  # 替换为你的实际路径\n",
    "            trust_remote_code=True,\n",
    "            truncation_side='right',\n",
    "            padding_side='right'\n",
    "        )\n",
    "        # emb\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # 编码器\n",
    "        # self.encoder = EncoderLayer(d_model, num_heads, dim_feedforward, dropout)\n",
    "        # 解码器层\n",
    "        self.decoder = Decoder(num_decoder_layers, d_model, num_heads, dim_feedforward, dropout, vocab_size, max_seq_length)\n",
    "\n",
    "    def forward(self, src_input_ids, encoder_outputs, src_mask, tgt_mask):\n",
    "\n",
    "        # 模拟编码器输出（随机初始化） !!!!!!!!!!!!!!!!!!!!\n",
    "        encoder_outputs = torch.randn(src_input_ids.size(0), src_input_ids.size(1), 1536)  # [batch_size, seq_len_src, d_model]\n",
    "        # 编码器的输入2：调用编码器，调用的编码器的输入是 [batch_size, seq_len, d_model]\n",
    "        # x = self.embedding(src_input_ids) * math.sqrt(self.embedding.embedding_dim)\n",
    "\n",
    "        # encoder_outputs = self.encoder(x)\n",
    "\n",
    "        output, attn_weights = self.decoder(src_input_ids, encoder_outputs, src_mask, tgt_mask)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "    def generate(self, start_token, max_len, src_input_ids, src_mask, tgt_mask):\n",
    "        with torch.no_grad():\n",
    "            # 确保 start_token 是整数类型，并初始化生成序列\n",
    "            if not isinstance(start_token, int):\n",
    "                raise ValueError(\"start_token must be an integer representing a valid token ID.\")\n",
    "            generated_sequence = [start_token]\n",
    "\n",
    "            for i in range(max_len - 1):  # 减一因为已经包含了一个 start_token\n",
    "                # 构造当前的目标序列张量，确保所有元素都是整数类型\n",
    "                tgt_tensor = torch.tensor(generated_sequence, dtype=torch.long).unsqueeze(0).to(\n",
    "                    next(self.parameters()).device)\n",
    "\n",
    "                # 检查生成的 tgt_tensor 是否包含有效的 token 索引\n",
    "                if tgt_tensor.max() >= self.embedding.num_embeddings:\n",
    "                    raise ValueError(\n",
    "                        f\"Generated token index {tgt_tensor.max().item()} exceeds embedding size {self.embedding.num_embeddings}.\")\n",
    "\n",
    "                # 通过 forward 函数获取解码器输出\n",
    "                output = self(src_input_ids, tgt_tensor, src_mask, tgt_mask)  # 使用 self(...) 而不是 self.forward(...)\n",
    "                print(output.size())\n",
    "                # 从输出中选择概率最大的 token ID，并确保它是整数类型\n",
    "                next_token = int(output.argmax(dim=-1)[:, -1].item())\n",
    "\n",
    "                # 将下一个 token 添加到生成序列中\n",
    "                generated_sequence.append(next_token)\n",
    "\n",
    "                # 如果遇到了结束标记，则停止生成\n",
    "                if next_token == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "            return generated_sequence\n",
    "\n",
    "    @classmethod\n",
    "    def generate_square_subsequent_mask(cls, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ],
   "id": "3062be4fff00e195",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:51:53.805990Z",
     "start_time": "2024-12-14T02:51:53.043819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tips： 测试 Transformer\n",
    "def simple_test_transformer_model():\n",
    "# 初始化模型\n",
    "    model = TransformerModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    # batch_size = 2\n",
    "    source_texts = [\"Translate this sentence.\", \"Another example sentence, final example sentence.\"]\n",
    "\n",
    "    # 关注 src_input_ids, src_mask 和 tgt_mask\n",
    "    # 分词\n",
    "    tokenized_source = tokenizer(source_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    src_input_ids = tokenized_source[\"input_ids\"]  # [batch, seq_len]\n",
    "    src_attention_mask = tokenized_source[\"attention_mask\"]\n",
    "\n",
    "    ## 目标序列掩码（decoder self-attention） causal mask 不看当前位置之后\n",
    "    seq_len = src_input_ids.size(1)\n",
    "    # 下三角包括对角线全为1\n",
    "    tgt_mask = torch.torch.tril(torch.ones(seq_len, seq_len))\n",
    "    # tgt_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)  # [seq_len, seq_len]\n",
    "    tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(1).to(src_input_ids.device)  # [1, 1, seq_len, seq_len]\n",
    "    tgt_mask = tgt_mask.expand(batch_size, -1, -1, -1)  # [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "    ## padding mask 不看padding\n",
    "    src_mask = src_attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "    src_mask = src_mask.expand(-1, num_heads, -1, -1)  # [batch_size, num_heads, 1, seq_len]\n",
    "    #\n",
    "    # print(f\"src_input_ids:{src_input_ids}\")\n",
    "    # print(f\"src_input_ids_size: {src_input_ids.size()}\")\n",
    "    # print(f\"tgt_mask: {tgt_mask}\")\n",
    "    # print(f\"tgt_mask_size: {tgt_mask.size()}\")\n",
    "    # print(f\"src_mask: {src_mask}\")\n",
    "    # print(f\"src_mask_size:{src_mask.size()}\")\n",
    "    # 前向传播测试 测试 Transformer 的 forward 函数成功\n",
    "    # 第一个参数是decoder的input。第二个参数不重要。第三个参数是防止padding参与运算，第四个参数是自回归掩码\n",
    "    output, attn_weights = model(src_input_ids, src_input_ids, src_mask, tgt_mask)\n",
    "    print(\"Forward pass output shape:\", output.size())  # 应为 (batch_size, seq_length, vocab_size)\n",
    "\n",
    "# 运行测试函数\n",
    "simple_test_transformer_model()\n"
   ],
   "id": "d80fd1294ba5fce0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.size: torch.Size([2, 10, 1536])\n",
      "k.size: torch.Size([2, 10, 1536])\n",
      "v.size: torch.Size([2, 10, 1536])\n",
      "new_q.size: torch.Size([2, 8, 10, 192])\n",
      "new_k.size: torch.Size([2, 8, 10, 192])\n",
      "new_v.size: torch.Size([2, 8, 10, 192])\n",
      "q.size: torch.Size([2, 10, 1536])\n",
      "k.size: torch.Size([2, 10, 1536])\n",
      "v.size: torch.Size([2, 10, 1536])\n",
      "new_q.size: torch.Size([2, 8, 10, 192])\n",
      "new_k.size: torch.Size([2, 8, 10, 192])\n",
      "new_v.size: torch.Size([2, 8, 10, 192])\n",
      "q.size: torch.Size([2, 10, 1536])\n",
      "k.size: torch.Size([2, 10, 1536])\n",
      "v.size: torch.Size([2, 10, 1536])\n",
      "new_q.size: torch.Size([2, 8, 10, 192])\n",
      "new_k.size: torch.Size([2, 8, 10, 192])\n",
      "new_v.size: torch.Size([2, 8, 10, 192])\n",
      "q.size: torch.Size([2, 10, 1536])\n",
      "k.size: torch.Size([2, 10, 1536])\n",
      "v.size: torch.Size([2, 10, 1536])\n",
      "new_q.size: torch.Size([2, 8, 10, 192])\n",
      "new_k.size: torch.Size([2, 8, 10, 192])\n",
      "new_v.size: torch.Size([2, 8, 10, 192])\n",
      "Forward pass output shape: torch.Size([2, 10, 30522])\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:51:55.612815Z",
     "start_time": "2024-12-14T02:51:55.608808Z"
    }
   },
   "cell_type": "code",
   "source": "# tips： 下面不是必要要求",
   "id": "c6c741c4a452a6f8",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:51:56.155765Z",
     "start_time": "2024-12-14T02:51:56.152634Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3ddfb952833bdba1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:51:57.274391Z",
     "start_time": "2024-12-14T02:51:56.690932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 推理的时候进行单条单条的推理\n",
    "def simple_test_inference_transformer_model():\n",
    "    import torch\n",
    "\n",
    "    # 参数设置\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    d_model = 1536\n",
    "    nhead = 4\n",
    "    num_decoder_layers = 2\n",
    "    dim_feedforward = 256\n",
    "    max_seq_length = 5\n",
    "    dropout = 0.1\n",
    "\n",
    "    # 初始化模型\n",
    "    model = TransformerModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    source_texts = [\"Translate this sentence.\"]\n",
    "    target_texts = [\"Translate this sentence.\"]\n",
    "    \n",
    "    # 准备两个tokenizer 和 两个掩码\n",
    "    # 分词\n",
    "    tokenized_source = tokenizer(source_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    tokenized_target = tokenizer(target_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    src_input_ids = tokenized_source[\"input_ids\"]  # [batch, seq_len]\n",
    "    tgt_input_ids = tokenized_target[\"input_ids\"]  # [batch, seq_len]\n",
    "    \n",
    "    src_attention_mask = tokenized_source[\"attention_mask\"]\n",
    "    tgt_attention_mask = tokenized_target[\"attention_mask\"]\n",
    "    \n",
    "    ## 生成掩码 padding mask 不看padding\n",
    "    src_mask = src_attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "    src_mask = src_mask.expand(-1, num_heads, -1, -1)  # [batch_size, num_heads, 1, seq_len]\n",
    "    \n",
    "    ## 目标序列掩码（decoder self-attention） causal mask 不看当前位置之后\n",
    "    seq_len = tgt_input_ids.size(1)\n",
    "    # 上三角对角矩阵 mask 矩阵来说不是 0 就是 1\n",
    "    tgt_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)  # [seq_len, seq_len]\n",
    "    tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(1).to(tgt_input_ids.device)  # [1, 1, seq_len, seq_len]\n",
    "    # tgt_mask = tgt_mask.expand(batch_size, -1, -1, -1)\n",
    "    \n",
    "    print(src_input_ids.size())\n",
    "    print(tgt_input_ids.size())\n",
    "    print(src_mask.size())\n",
    "    print(tgt_mask.size())\n",
    "    # 前向传播测试 测试 Transformer 的 forward 函数成功\n",
    "    output = model(src_input_ids, tgt_input_ids, src_mask, tgt_mask)\n",
    "    print(\"Forward pass output shape:\", output.shape)  # 应为 (batch_size, seq_length, vocab_size)\n",
    "\n",
    "    \n",
    "    # 设置起始 token\n",
    "    start_token = tokenizer.cls_token_id if tokenizer.cls_token_id is not None else 1  # 假设 cls_token 作为起始 token\n",
    "\n",
    "    # 调用生成函数\n",
    "    print(\"Generation test:\")\n",
    "    generated_seq = model.generate(start_token=start_token, max_len=max_seq_length, \n",
    "                                   src_input_ids=src_input_ids, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "    # 检查生成的序列\n",
    "    print(\"Generated token sequence:\", generated_seq)\n",
    "\n",
    "    # 反编码为文本\n",
    "    generated_text = tokenizer.decode(generated_seq, skip_special_tokens=True)\n",
    "    print(\"Generated text:\", generated_text)\n",
    "    \n",
    "    \n",
    "    # # 模拟输入数据\n",
    "    # tgt = torch.randint(0, vocab_size, (2, max_seq_length))  # 批大小2，序列长度为max_seq_length\n",
    "    # enc_output = torch.rand(2, max_seq_length, d_model)  # 假设编码器的输出\n",
    "    # src_mask = None  # 示例中不使用具体的mask\n",
    "    # tgt_mask = TransformerModel.generate_square_subsequent_mask(max_seq_length)\n",
    "    # \n",
    "\n",
    "    # 生成测试\n",
    "    ## 设置起始 token\n",
    "    start_token = 1\n",
    "\n",
    "    # 假设编码器的输出\n",
    "    enc_output = torch.rand(2, max_seq_length, d_model)\n",
    "    print(enc_output.size())\n",
    "\n",
    "    # 调用生成函数\n",
    "    print(1)\n",
    "    ## 从 start_token 开始生成内容 start_token: max_seq_length: enc_output: \n",
    "    # 有没有必要设置 src_mask, tgt_mask？？？？？\n",
    "    generated_seq = model.generate(max_seq_length, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "    # 检查生成的序列\n",
    "    print(\"Generated token sequence:\", generated_seq)\n",
    "    \n",
    "    print(4)\n",
    "    # 反编码为文本\n",
    "    generated_text = model.tokenizer.decode(generated_seq, skip_special_tokens=True)\n",
    "    print(\"Generated text:\", generated_text)\n",
    "\n",
    "# 运行测试函数\n",
    "simple_test_transformer_model()\n"
   ],
   "id": "564499aef2d2abc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.size: torch.Size([2, 10, 1536])\n",
      "k.size: torch.Size([2, 10, 1536])\n",
      "v.size: torch.Size([2, 10, 1536])\n",
      "new_q.size: torch.Size([2, 8, 10, 192])\n",
      "new_k.size: torch.Size([2, 8, 10, 192])\n",
      "new_v.size: torch.Size([2, 8, 10, 192])\n",
      "q.size: torch.Size([2, 10, 1536])\n",
      "k.size: torch.Size([2, 10, 1536])\n",
      "v.size: torch.Size([2, 10, 1536])\n",
      "new_q.size: torch.Size([2, 8, 10, 192])\n",
      "new_k.size: torch.Size([2, 8, 10, 192])\n",
      "new_v.size: torch.Size([2, 8, 10, 192])\n",
      "q.size: torch.Size([2, 10, 1536])\n",
      "k.size: torch.Size([2, 10, 1536])\n",
      "v.size: torch.Size([2, 10, 1536])\n",
      "new_q.size: torch.Size([2, 8, 10, 192])\n",
      "new_k.size: torch.Size([2, 8, 10, 192])\n",
      "new_v.size: torch.Size([2, 8, 10, 192])\n",
      "q.size: torch.Size([2, 10, 1536])\n",
      "k.size: torch.Size([2, 10, 1536])\n",
      "v.size: torch.Size([2, 10, 1536])\n",
      "new_q.size: torch.Size([2, 8, 10, 192])\n",
      "new_k.size: torch.Size([2, 8, 10, 192])\n",
      "new_v.size: torch.Size([2, 8, 10, 192])\n",
      "Forward pass output shape: torch.Size([2, 10, 30522])\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:51:58.820355Z",
     "start_time": "2024-12-14T02:51:58.815963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        tgt_input = batch['input'].to(device)\n",
    "        tgt_output = batch['output'].to(device)\n",
    "        enc_output = batch.get('enc_output', None)  # 如果有编码器输出\n",
    "        src_mask = batch.get('src_mask', None)\n",
    "        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(tgt_input, enc_output, src_mask, tgt_mask)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt_output.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "7803199314febfb",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T11:59:35.990732Z",
     "start_time": "2024-12-07T11:59:35.987327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_mock_data(batch_size, seq_length, vocab_size):\n",
    "    # 创建随机输入序列，保证 token ID 在 [0, vocab_size - 1] 范围内\n",
    "    input_sequences = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "    output_sequences = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "    \n",
    "    # 其他部分保持不变...\n",
    "    enc_output = torch.randn(batch_size, seq_length, d_model)  # 示例编码器输出\n",
    "    src_mask = torch.ones(batch_size, 1, seq_length)           # 示例源序列掩码\n",
    "    \n",
    "    return {\n",
    "        'input': input_sequences,\n",
    "        'output': output_sequences,\n",
    "        'enc_output': enc_output,\n",
    "        'src_mask': src_mask\n",
    "    }"
   ],
   "id": "aa3077370ee31f56",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:52:09.451642Z",
     "start_time": "2024-12-14T02:52:09.446824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tips：三种掩码方式：padding mask, causal mask, combined mask\n",
    "import torch\n",
    "\n",
    "# 假设输入数据\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "num_heads = 8\n",
    "\n",
    "# 假设填充 token ID 是 0\n",
    "input_ids = torch.tensor([\n",
    "    [1, 2, 3, 0, 0],  # 第一个序列，最后两个是填充\n",
    "    [4, 5, 6, 7, 0]   # 第二个序列，最后一个是填充\n",
    "])\n",
    "# 1. 构造 Padding Mask # padding_mask: 1 表示有效位置，0 表示填充位置\n",
    "padding_mask = (input_ids != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "# 2. 构造 Causal Mask\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()  # (seq_len, seq_len)\n",
    "causal_mask = causal_mask.unsqueeze(0).expand(batch_size, -1, -1)  # (batch_size, seq_len, seq_len)\n",
    "print(causal_mask)\n",
    "\n",
    "print(padding_mask)\n",
    "# 3. Combine Masks\n",
    "# 使用或 运算符 \n",
    "combined_mask = causal_mask | ~padding_mask.squeeze(1).expand(-1, seq_len, -1)  # (batch_size, seq_len, seq_len)\n",
    "print(combined_mask)"
   ],
   "id": "8caee1453995dad4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False, False]]])\n",
      "tensor([[[[ True,  True,  True, False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True,  True,  True, False]]]])\n",
      "tensor([[[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True]]])\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e1c4908f82601af2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
