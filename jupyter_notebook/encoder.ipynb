{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8fd0db9",
   "metadata": {},
   "source": [
    "总结：首先是Input_embedding和positional_encoding 是Transformer中的一环\n",
    "EncoderLayer包含MultiHeadAttention\n",
    "https://blog.csdn.net/weixin_44613415/article/details/139848359"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567925a",
   "metadata": {},
   "source": [
    "## 下面是Transformer的class\n",
    "已知这里会有Input_embedding和Positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "91f62c68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.042268Z",
     "start_time": "2024-11-27T11:47:50.038665Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "3fc86123",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.058549Z",
     "start_time": "2024-11-27T11:47:50.053273Z"
    }
   },
   "source": [
    "class PositionalEncoding(nn.Module):#维度是奇数也不会报错\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()#继承父类nn.Module\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        # 只考虑偶数位置，确保 div_term 的长度匹配\n",
    "        div_term = torch.exp(2 * torch.arange(0, (d_model + 1) // 2).float()  * -(math.log(10000.0) / d_model))\n",
    "        #其中2 * torch.arange(0, (d_model + 1) // 2).float()是2i\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[:d_model // 2]) \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))#pe是[batch_size, seq_length, d_model]，注册后会成为self.pe\n",
    "\n",
    "    def forward(self, x):#这里的x是[batch_size, seq_length, d_model]\n",
    "        return x + self.pe[:, :x.size(1)]#也可以尝试下除了相加的方式，但是感觉乘法的话就会有权重为0的可能性"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "4df05750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.079183Z",
     "start_time": "2024-11-27T11:47:50.074010Z"
    }
   },
   "source": [
    "class MultiHeadAttention(nn.Module):#x是[batch_size, seq_length, d_model]\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()#继承父类nn.Module\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = self.d_model//self.num_heads #因为要整除\n",
    "        self.Qw = nn.Linear(d_model, d_model)#加载Q权重\n",
    "        self.Kw = nn.Linear(d_model, d_model)#加载K权重\n",
    "        self.Vw = nn.Linear(d_model, d_model)#加载V权重\n",
    "        self.Ow = nn.Linear(d_model, d_model)#加载V权重\n",
    "        \n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    #把第一列和第二列交换，也就是seq_length和self.num_heads\n",
    "    #变成了(batch_size, self.num_heads, seq_length, self.d_k)\n",
    "\n",
    "    def concat_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, d_k = x.size()\n",
    "        return x.transpose(2,1).reshape(batch_size, seq_length, self.d_model)\n",
    "    \n",
    "    def dot_attention(self, Q, K, V):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)#首先是K转置，所以要交换最后两列，矩阵相乘\n",
    "        #得到格式为[batch_size,num_heads,seq_length,seq_length]\n",
    "\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)#对最后一维做softmax\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output#得到了(batch_size, self.num_heads, seq_length, self.d_k)\n",
    "\n",
    "    \n",
    "    def forward(self,Q,K,V):#首先第一步是分头，把d_model分解成num_heads\n",
    "        Qa=self.split_heads(self.Qw(Q))\n",
    "        Ka=self.split_heads(self.Kw(K))\n",
    "        Va=self.split_heads(self.Vw(V))\n",
    "        attn_output = self.dot_attention(Qa, Ka, Va)\n",
    "        output= self.Ow(self.concat_heads(attn_output))\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "9d1b8a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.098055Z",
     "start_time": "2024-11-27T11:47:50.094059Z"
    }
   },
   "source": [
    "class FeedForward(nn.Module):# 感觉前馈层能做很多，加上dropout吧\n",
    "    def __init__(self, d_model,d_hidden,is_drop = True, drop = 0.1):\n",
    "        super().__init__()#继承父类nn.Module\n",
    "        self.W1 = nn.Linear(d_model, d_hidden)\n",
    "        self.W2 = nn.Linear(d_hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.is_drop=is_drop\n",
    "    \n",
    "    def forward(self, x,):\n",
    "        x = self.relu(self.W1(x))\n",
    "        if self.is_drop:\n",
    "            x=self.dropout(x)\n",
    "        return self.W2(x)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "712ef3c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.116692Z",
     "start_time": "2024-11-27T11:47:50.113373Z"
    }
   },
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model, num_heads, d_hidden,is_drop = True, drop = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = FeedForward(d_model,d_hidden,is_drop, drop)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attn(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        f_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + f_output)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "dd6d83f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.134651Z",
     "start_time": "2024-11-27T11:47:50.131651Z"
    }
   },
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        #Embedding层输入（vocab表，embedding的维度），输出（句子的长度，embedding的维度）\n",
    "        self.encoder_input_embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        #PositionalEncoding层输入（最大长度，嵌入维度）先把位置编码固定了\n",
    "        self.positional_encoding = PositionalEncoding(max_seq_length, embedding_dim)#位置编码的维度和嵌入维度通常要一样，因为要相加在一起\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "7276d249",
   "metadata": {},
   "source": [
    "# 测试部分"
   ]
  },
  {
   "cell_type": "code",
   "id": "679e2768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.154377Z",
     "start_time": "2024-11-27T11:47:50.149376Z"
    }
   },
   "source": [
    "## tips：下面是关于embedding层的测试\n",
    "# 定义词汇表（Vocabulary）\n",
    "vocab = {'I': 0, 'like': 1, 'to': 2, 'learn': 3, 'deep': 4, 'learning': 5, 'with': 6, 'PyTorch': 7}\n",
    "vocab_size = len(vocab)\n",
    "# 定义句子\n",
    "sentence = ['I', 'like', 'to', 'learn', 'deep', 'learning', 'with', 'PyTorch']\n",
    "\n",
    "# 将句子中的单词映射为索引序列\n",
    "sentence_indices = [vocab[word] for word in sentence]\n",
    "# 将索引序列转换为张量，同事\n",
    "sentence_tensor = torch.tensor(sentence_indices).unsqueeze(0) \n",
    "# 定义 nn.Embedding 层\n",
    "embedding_dim = 6\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "# 使用 nn.Embedding 将索引序列转换为嵌入向量\n",
    "embedded_sentence = embedding_layer(sentence_tensor)\n",
    "# 打印结果\n",
    "print(\"原始句子:\", sentence)\n",
    "print(\"句子索引:\", sentence_tensor )\n",
    "print(\"嵌入向量形状:\", embedded_sentence.shape)  # (sequence_length, embedding_dim)\n",
    "print(\"嵌入向量:\\n\", embedded_sentence)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始句子: ['I', 'like', 'to', 'learn', 'deep', 'learning', 'with', 'PyTorch']\n",
      "句子索引: tensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "嵌入向量形状: torch.Size([1, 8, 6])\n",
      "嵌入向量:\n",
      " tensor([[[ 0.3950, -1.2595,  0.5595, -1.2674,  2.7538, -0.8248],\n",
      "         [ 1.7810,  0.4843, -0.5217,  0.6411,  2.0571,  0.8603],\n",
      "         [-1.0124,  1.2623, -0.0644,  1.2093,  0.0174,  0.3585],\n",
      "         [-1.2773, -0.6560, -0.3979, -0.9609,  1.4672,  1.4918],\n",
      "         [-0.3171,  0.5181,  1.0291,  1.4574, -0.9940,  1.2083],\n",
      "         [-2.9706, -0.8443, -0.9328,  0.6566,  0.2987, -0.1731],\n",
      "         [-0.3194,  1.1842, -1.3125,  0.6940,  0.2855,  0.4416],\n",
      "         [ 0.5895,  0.1618, -0.8739, -0.3380,  0.0239,  0.2934]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "1fb5a03c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.174965Z",
     "start_time": "2024-11-27T11:47:50.169330Z"
    }
   },
   "source": [
    "## tips：下面是关于positional embedding层的测试\n",
    "max_seq_length = 10\n",
    "d_model = 6\n",
    "pe = torch.zeros(max_seq_length, d_model)\n",
    "position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "# 只考虑偶数位置，确保 div_term 的长度匹配\n",
    "div_term = torch.exp(2*torch.arange(0, (d_model + 1) // 2).float()  * -(math.log(10000.0) / d_model))\n",
    "# print(position)\n",
    "# print(torch.arange(0, (d_model + 1) // 2).float(), div_term)\n",
    "# 分别对偶数和奇数位置赋值\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term[:d_model // 2]) \n",
    "pe=pe.unsqueeze(0)#维度变成[batch_size, seq_length, d_model]\n",
    "embedded_sentence + pe[:, :embedded_sentence.size(1)]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3950, -0.2595,  0.5595, -0.2674,  2.7538,  0.1752],\n",
       "         [ 2.6224,  1.0246, -0.4753,  1.6400,  2.0592,  1.8603],\n",
       "         [-0.1031,  0.8462,  0.0283,  2.2050,  0.0217,  1.3585],\n",
       "         [-1.1362, -1.6459, -0.2591,  0.0294,  1.4737,  2.4918],\n",
       "         [-1.0740, -0.1356,  1.2137,  2.4402, -0.9854,  2.2082],\n",
       "         [-3.9295, -0.5606, -0.7028,  1.6298,  0.3094,  0.8268],\n",
       "         [-0.5989,  2.1443, -1.0376,  1.6554,  0.2984,  1.4415],\n",
       "         [ 1.2465,  0.9157, -0.5546,  0.6097,  0.0390,  1.2933]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "674bf768",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.193714Z",
     "start_time": "2024-11-27T11:47:50.188104Z"
    }
   },
   "source": [
    "## tips：下面是关于维度转换的测试\n",
    "batch_size = 2\n",
    "seq_length = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "# 定义输入张量\n",
    "x = torch.tensor([\n",
    "    [\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        [9, 10, 11, 12, 13, 14, 15, 16],\n",
    "        [17, 18, 19, 20, 21, 22, 23, 24],\n",
    "        [25, 26, 27, 28, 29, 30, 31, 32]\n",
    "    ],\n",
    "    [\n",
    "        [33, 34, 35, 36, 37, 38, 39, 40],\n",
    "        [41, 42, 43, 44, 45, 46, 47, 48],\n",
    "        [49, 50, 51, 52, 53, 54, 55, 56],\n",
    "        [57, 58, 59, 60, 61, 62, 63, 64]\n",
    "    ]\n",
    "])  # 形状 [2, 4, 8]\n",
    "\n",
    "# 重塑张量\n",
    "x = x.view(batch_size, seq_length, num_heads, d_k)  # [2, 4, 2, 4]\n",
    "print(\"原张量格式如下：\",x)\n",
    "# 调整维度顺序\n",
    "y = x.permute(0, 2, 1, 3)  # [2, 2, 4, 4]\n",
    "# 打印新张量的形状\n",
    "print(\"使用permute后的格式如下：\",y)\n",
    "z = x.transpose(1,2)\n",
    "zz = x.transpose(2,1)\n",
    "print(\"使用transpose后的格式如下：\",z)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原张量格式如下： tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8]],\n",
      "\n",
      "         [[ 9, 10, 11, 12],\n",
      "          [13, 14, 15, 16]],\n",
      "\n",
      "         [[17, 18, 19, 20],\n",
      "          [21, 22, 23, 24]],\n",
      "\n",
      "         [[25, 26, 27, 28],\n",
      "          [29, 30, 31, 32]]],\n",
      "\n",
      "\n",
      "        [[[33, 34, 35, 36],\n",
      "          [37, 38, 39, 40]],\n",
      "\n",
      "         [[41, 42, 43, 44],\n",
      "          [45, 46, 47, 48]],\n",
      "\n",
      "         [[49, 50, 51, 52],\n",
      "          [53, 54, 55, 56]],\n",
      "\n",
      "         [[57, 58, 59, 60],\n",
      "          [61, 62, 63, 64]]]])\n",
      "使用permute后的格式如下： tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 9, 10, 11, 12],\n",
      "          [17, 18, 19, 20],\n",
      "          [25, 26, 27, 28]],\n",
      "\n",
      "         [[ 5,  6,  7,  8],\n",
      "          [13, 14, 15, 16],\n",
      "          [21, 22, 23, 24],\n",
      "          [29, 30, 31, 32]]],\n",
      "\n",
      "\n",
      "        [[[33, 34, 35, 36],\n",
      "          [41, 42, 43, 44],\n",
      "          [49, 50, 51, 52],\n",
      "          [57, 58, 59, 60]],\n",
      "\n",
      "         [[37, 38, 39, 40],\n",
      "          [45, 46, 47, 48],\n",
      "          [53, 54, 55, 56],\n",
      "          [61, 62, 63, 64]]]])\n",
      "使用transpose后的格式如下： tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 9, 10, 11, 12],\n",
      "          [17, 18, 19, 20],\n",
      "          [25, 26, 27, 28]],\n",
      "\n",
      "         [[ 5,  6,  7,  8],\n",
      "          [13, 14, 15, 16],\n",
      "          [21, 22, 23, 24],\n",
      "          [29, 30, 31, 32]]],\n",
      "\n",
      "\n",
      "        [[[33, 34, 35, 36],\n",
      "          [41, 42, 43, 44],\n",
      "          [49, 50, 51, 52],\n",
      "          [57, 58, 59, 60]],\n",
      "\n",
      "         [[37, 38, 39, 40],\n",
      "          [45, 46, 47, 48],\n",
      "          [53, 54, 55, 56],\n",
      "          [61, 62, 63, 64]]]])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "8c1ed97a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.218898Z",
     "start_time": "2024-11-27T11:47:50.208939Z"
    }
   },
   "source": [
    "## tips：下面是关于多头注意力的测试\n",
    "batch_size = 2\n",
    "seq_length = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "\n",
    "# 实例化类\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# 创建随机张量\n",
    "Q = torch.randn(batch_size, seq_length, d_model)\n",
    "K = torch.randn(batch_size, seq_length, d_model)\n",
    "V = torch.randn(batch_size, seq_length, d_model)\n",
    "\n",
    "# 测试 split_heads 和 concat_heads\n",
    "split = mha.split_heads(Q)\n",
    "concat = mha.concat_heads(split)\n",
    "assert torch.allclose(Q, concat), \"split_heads 和 concat_heads 不是互逆的。\"\n",
    "\n",
    "# 测试 scaled_dot_product_attention\n",
    "attn_output = mha.dot_attention(split, split, split)\n",
    "expected_shape = (batch_size, num_heads, seq_length, mha.d_k)\n",
    "assert attn_output.shape == expected_shape, f\"Attention 输出形状不正确，期望 {expected_shape}，得到 {attn_output.shape}。\"\n",
    "\n",
    "# 测试 forward 方法\n",
    "output = mha(Q, K, V)\n",
    "expected_shape = (batch_size, seq_length, d_model)\n",
    "assert output.shape == expected_shape, f\"输出形状不正确，期望 {expected_shape}，得到 {output.shape}。\"\n",
    "\n",
    "# 检查是否有运行时错误\n",
    "try:\n",
    "    output = mha(Q, K, V)\n",
    "    print(\"前向传播成功。\")\n",
    "except Exception as e:\n",
    "    print(f\"前向传播时出错：{e}\")\n",
    "\n",
    "# 验证梯度是否正确传播\n",
    "Q.requires_grad_(True)\n",
    "output = mha(Q, K, V)\n",
    "output.mean().backward()\n",
    "assert Q.grad is not None, \"梯度没有回传到 Q。\"\n",
    "\n",
    "print(\"所有测试通过！\")\n",
    "\n",
    "# 与 PyTorch 的实现进行比较\n",
    "torch_mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "# 将自定义 MHA 的权重复制到 PyTorch MHA\n",
    "with torch.no_grad():\n",
    "    torch_mha.in_proj_weight = nn.Parameter(torch.cat([\n",
    "        mha.Qw.weight,\n",
    "        mha.Kw.weight,\n",
    "        mha.Vw.weight\n",
    "    ], dim=0))\n",
    "    torch_mha.in_proj_bias = nn.Parameter(torch.cat([\n",
    "        mha.Qw.bias,\n",
    "        mha.Kw.bias,\n",
    "        mha.Vw.bias\n",
    "    ], dim=0))\n",
    "    torch_mha.out_proj.weight = mha.Ow.weight\n",
    "    torch_mha.out_proj.bias = mha.Ow.bias\n",
    "\n",
    "# 使用 PyTorch 的 MHA\n",
    "torch_output, _ = torch_mha(Q, K, V)\n",
    "\n",
    "# 比较输出\n",
    "if torch.allclose(output, torch_output, atol=1e-6):\n",
    "    print(\"自定义实现与 PyTorch 实现输出匹配。\")\n",
    "else:\n",
    "    print(\"自定义实现与 PyTorch 实现输出不匹配。\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向传播成功。\n",
      "所有测试通过！\n",
      "自定义实现与 PyTorch 实现输出匹配。\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "f3cfc435",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.238564Z",
     "start_time": "2024-11-27T11:47:50.233105Z"
    }
   },
   "source": [
    "# tips：下面是关于Norm的测试\n",
    "\n",
    "# 输入张量\n",
    "x = torch.tensor([\n",
    "    [[1.0, 2.0, 3.0, 4.0],\n",
    "     [5.0, 6.0, 7.0, 8.0],\n",
    "     [9.0, 10.0, 11.0, 12.0]],\n",
    "    [[13.0, 14.0, 15.0, 16.0],\n",
    "     [17.0, 18.0, 19.0, 20.0],\n",
    "     [21.0, 22.0, 23.0, 24.0]]\n",
    "])#2，3，4\n",
    "\n",
    "# BatchNorm，假设特征维度为 4\n",
    "batch_norm = nn.BatchNorm1d(num_features=4)\n",
    "x_bn = x.permute(0, 2, 1)  # 将维度调整为 (batch, features, seq)，因为BatchNorm1d默认第一维是特征\n",
    "output_bn = batch_norm(x_bn)\n",
    "x_original = output_bn .permute(0, 2, 1)\n",
    "print(\"BatchNorm 输出：\\n\", x_original)\n",
    "\n",
    "# LayerNorm\n",
    "layer_norm = nn.LayerNorm(normalized_shape=4)\n",
    "output_ln = layer_norm(x)\n",
    "print(\"LayerNorm 输出：\\n\", output_ln)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm 输出：\n",
      " tensor([[[-1.4639, -1.4639, -1.4639, -1.4639],\n",
      "         [-0.8783, -0.8783, -0.8783, -0.8783],\n",
      "         [-0.2928, -0.2928, -0.2928, -0.2928]],\n",
      "\n",
      "        [[ 0.2928,  0.2928,  0.2928,  0.2928],\n",
      "         [ 0.8783,  0.8783,  0.8783,  0.8783],\n",
      "         [ 1.4639,  1.4639,  1.4639,  1.4639]]], grad_fn=<PermuteBackward0>)\n",
      "LayerNorm 输出：\n",
      " tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416]],\n",
      "\n",
      "        [[-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "1c27dcfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.297508Z",
     "start_time": "2024-11-27T11:47:50.250893Z"
    }
   },
   "source": [
    "# tips：下面是关于FeedForward的测试\n",
    "ffn = FeedForward(512, 2048)\n",
    "input_tensor = torch.randn(32, 128, 512)\n",
    "output_tensor = ffn(input_tensor)\n",
    "print(output_tensor.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 512])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "71f2d1de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T09:14:51.968120Z",
     "start_time": "2024-12-05T09:14:51.810661Z"
    }
   },
   "source": [
    "# tips：下面是关于encoder的测试\n",
    "d_model = 8\n",
    "num_heads = 4\n",
    "d_hidden = 128\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Initialize the EncoderLayer\n",
    "encoder_layer = EncoderLayer(d_model, num_heads, d_hidden)\n",
    "\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass through the encoder layer\n",
    "output = encoder_layer(x)\n",
    "\n",
    "# Verify the output shape\n",
    "print(\"Input shape:\", x.shape,x)\n",
    "print(\"Output shape:\", output.shape,output)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EncoderLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Initialize the EncoderLayer\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m encoder_layer \u001B[38;5;241m=\u001B[39m \u001B[43mEncoderLayer\u001B[49m(d_model, num_heads, d_hidden)\n\u001B[0;32m     11\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(batch_size, seq_len, d_model)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Forward pass through the encoder layer\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'EncoderLayer' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "52200c27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.415056Z",
     "start_time": "2024-11-27T11:47:50.412042Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80fa34ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.433866Z",
     "start_time": "2024-11-27T11:47:50.431821Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "edd28f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:47:50.451684Z",
     "start_time": "2024-11-27T11:47:50.449383Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
