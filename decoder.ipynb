{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![encoder-decoder](./decoder_img/encoder-decoder.png)\n",
   "id": "4bb5c87191ee0823"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T12:28:45.696172Z",
     "start_time": "2024-12-10T12:28:45.688798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import AutoTokenizer\n"
   ],
   "id": "baf14d9e89604217",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T12:28:47.521804Z",
     "start_time": "2024-12-10T12:28:47.453681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tokenizer 目录路径\n",
    "local_tokenizer_dir = \"./tokenizer_files/\"  # 替换为你的实际路径\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_tokenizer_dir,\n",
    "    trust_remote_code=True,  # 如果使用的分词器有自定义代码，需要启用此选项\n",
    "    truncation_side='right', # 设置分词器的截断侧\n",
    "    padding_side='right'     # 设置分词器的填充侧\n",
    ")"
   ],
   "id": "563978ee2481169d",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T12:28:54.748031Z",
     "start_time": "2024-12-10T12:28:54.742886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# parameter setting\n",
    "d_model = 512   # 输入的维度 Qwen/Qwen2.5-1.5B-Instruct 中是 hidden_size\n",
    "d_ff = 2048     # 前向传播隐藏层维度\n",
    "d_k = d_v = 64  # K(=Q), V的维度 \n",
    "n_layers = 6    # N of encoder and decoder\n",
    "n_heads = 8     # the number of Multi-Head Attention\n",
    "\n",
    "batch_size = 2 # 假设每个样本的长度为 5\n",
    "seq_len = 512 # 假设每个样本的长度为 5\n",
    "hidden_size = 1024\n",
    "num_heads = 8   # 多头注意力中的头数\n",
    "\n",
    "# 假设这些是我们所需的模型参数\n",
    "dropout = 0.1         # Dropout 比例\n",
    "vocab_size = 151936   # Qwen/Qwen2.5-1.5B-Instruct 的词汇表的大小\n",
    "num_layers = 6        # Decoder 中的层数\n",
    "\n",
    "# 定义嵌入层，词表大小和分词器一致\n",
    "embedding_dim = 512  # 嵌入维度，可根据需求调整\n",
    "max_seq_len = 512  # 最大序列长度\n"
   ],
   "id": "8f977da972ea1792",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在多头注意力中，每个注意力头的计算遵循**Scaled Dot-Product Attention**公式：\n",
    "\n",
    "#### **2.1. 单头注意力公式**\n",
    "\n",
    "对于每个头，计算方式为：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "- $Q$: 查询向量 (query)。\n",
    "- $K$: 键向量 (key)。\n",
    "- $V$: 值向量 (value)。\n",
    "- $d_k$: 每个头的键/查询向量维度。\n",
    "#### **2.2. 多头注意力公式**\n",
    "\n",
    "多头注意力通过多个头并行计算注意力，然后将结果合并：\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "- 每个注意力头的计算：\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "$W_i^Q, W_i^K, W_i^V$ 是每个头的权重矩阵。"
   ],
   "id": "9d34c9aff65d1733"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "828c3019ca77c41b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![formula](./decoder_img/formula_multi-head%20attention.png)",
   "id": "3040e90b321ac5f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2837cf4a7f333c23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![sdqa](./decoder_img/sdpa.png)\n",
    "![multi-head-attention](./decoder_img/multi-head%20attention.png)"
   ],
   "id": "67f1fc75d93df0c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cfe64e12799b2dbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:49:45.031592Z",
     "start_time": "2024-12-10T13:49:45.023884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # 定义线性变换层\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "    def split_head(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, head_dim).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # 线性变换\n",
    "        q = self.q_linear(q)  # (batch_size, seq_len_q, hidden_size)  对 emb 的值或者维度做了改变\n",
    "        k = self.k_linear(k)  # (batch_size, seq_len_k, hidden_size)\n",
    "        v = self.v_linear(v)  # (batch_size, seq_len_v, hidden_size)\n",
    "        print(f\"q.size: {q.size()}\")\n",
    "        print(f\"k.size: {k.size()}\")\n",
    "        print(f\"v.size: {v.size()}\")\n",
    "        \n",
    "        # 分割头\n",
    "        q = self.split_head(q, batch_size)  # (batch_size, num_heads, seq_len_q, head_dim)\n",
    "        k = self.split_head(k, batch_size)  # (batch_size, num_heads, seq_len_k, head_dim)\n",
    "        v = self.split_head(v, batch_size)  # (batch_size, num_heads, seq_len_v, head_dim)\n",
    "        print(f\"new_q.size: {q.size()}\")\n",
    "        print(f\"new_k.size: {k.size()}\")\n",
    "        print(f\"new_v.size: {v.size()}\")\n",
    "    \n",
    "        # 每头独立计算\n",
    "        ## 多头计算注意力分数 QK^T\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        if mask is not None:\n",
    "            # scores 张量中对应于 mask 中为0的位置（即不应该被关注的位置），将其值设置为 -1e9\n",
    "            # 这样-1e9在经过softmax以后得到的概率接近于零\n",
    "            # 实现了 mask 中掩码为0的位置是没有score分数的\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        ## 减少计算使用过设置为负无穷大，然后负无穷大的对应函数值为0，0不参与计算实现的\n",
    "        ## softmax 作用与最后一维，负无穷大对应的是 0，对应到 v 就不会有最后向量\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        ## 应用注意力权重到值向量\n",
    "        output = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, head_dim)\n",
    "\n",
    "        # 合并头\n",
    "        ######## 多头是如何实现的？？？？\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)  # (batch_size, seq_len_q, hidden_size)\n",
    "        # (batch_size, num_heads, seq_len, head_dim) 变化 transpose(1, 2)\n",
    "        # (batch_size, seq_len, num_heads, head_dim)\n",
    "        # view 之前用 contiguous() 是为了确保张量在内存中的布局是连续的\n",
    "        # view 进行重塑，将 num_heads 和 head_dim 合并在一起\n",
    "        \n",
    "        # 最后一个线性变换\n",
    "        output = self.o_linear(output)  # (batch_size, seq_len_q, hidden_size)\n",
    "\n",
    "        return output, attention_weights"
   ],
   "id": "e36ce09a6d3d1149",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![FFN](./decoder_img/FFN.png)",
   "id": "756d2b41394f38ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:44:57.937890Z",
     "start_time": "2024-12-10T13:44:57.934383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n"
   ],
   "id": "2ca36ed3738b96a0",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![pe](./decoder_img/position%20embedding.png)",
   "id": "a4b21fe4c71115f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T12:29:06.484955Z",
     "start_time": "2024-12-10T12:29:06.470186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 继承了PyTorch的 nn.Module 类，这意味着它可以像其他PyTorch模块一样被使用\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # 构造函数接收两个参数 d_model：嵌入向量大小，max_len：最长序列长度\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model) # (max_len, d_model) 存储位置编码\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # ①\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 偶数维赋值  # ②\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 奇数维赋值  # ②\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # [max_len, 1, d_model]\n",
    "        self.register_buffer('pe', pe) # 注册为一个缓冲区，被保存在模型的字典中，不会参与梯度更新\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # 调整位置编码大小：根据输入 x 的实际序列长度切片位置编码矩阵，确保只添加必要的部分。\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, vocab_size):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # 自注意力层（Causal-Attention）\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)     \n",
    "        # 编码器-解码器注意力层（Cross-Attention）\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # 前馈网络（Feed-Forward）\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        \n",
    "        # LayerNorm 层（Layer Normalization）\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout 层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # 将输入索引转换为嵌入向量，并调整尺度\n",
    "       \n",
    "        # 自注意力：输入为解码器自身的输入（x），q,k,v全部都是x \n",
    "        attn_output, block1 = self.self_attn(x, x, x, tgt_mask)  # [bs, seq_len, emb] [bs, 1, seq_len, seq_len]\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # 残差连接和 LayerNorm\n",
    "        \n",
    "        # 交叉注意力：输入为编码器输出（enc_output）和解码器输入（x）\n",
    "        # q：decoder's output. k + v: encoder's output \n",
    "        attn_output, block2 = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))  # 残差连接和 LayerNorm\n",
    "        \n",
    "        # 前馈网络（Feed-Forward）\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))  # 残差连接和 LayerNorm\n",
    "        \n",
    "        return x, block1, block2"
   ],
   "id": "d56752bc82caa324",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T12:29:07.624178Z",
     "start_time": "2024-12-10T12:29:07.618229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout, vocab_size, max_len=5000):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # 建立模型的 emb 层  创建一个形状为 (vocab_size, d_model) 的可训练参数矩阵\n",
    "        # 初始化：嵌入矩阵的权重通常会在初始化时随机分配，或者从预训练模型中加载。\n",
    "        # 训练：在训练过程中，嵌入矩阵的权重会通过反向传播进行更新，以优化模型性能。\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 位置编码\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
    "        \n",
    "        # 解码器层列表\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout, vocab_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attention_weights = {}\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 添加嵌入和位置编码 \n",
    "        ## 嵌入的扩大：乘以嵌入维度的平方根 嵌入向量的初始值通常是从一个较小的标准差分布中随机抽取的，比如标准正态分布或均匀分布。如果直接将这些小数值输入到后续的线性变换、激活函数等操作中，可能会导致信号逐渐衰减，特别是当网络层数较多时。通过乘以嵌入维度的平方根，可以放大这些初始值，使得它们在整个网络中的传播更为稳定。\n",
    "        # 在 Transformer 模型中，位置编码（positional encoding）会直接加到词嵌入（token embeddings）上。位置编码的设计通常是基于正弦和余弦函数，其幅度大致为 1。如果不调整词嵌入的规模，那么位置编码的影响可能会过大或过小，从而破坏了两者之间的相对比例。乘以 sqrt(d_model) 可以使词嵌入的均方根（RMS, Root Mean Square）与位置编码相近，维持两者在一个相似的数量级上，避免一方压倒另一方。\n",
    "        # 这种做法源自 Vaswani 等人在他们的论文《Attention Is All You Need》中提出的建议。他们指出，这样的缩放有助于保持模型各部分的输出具有相似的方差，进而促进更有效的学习。\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        ## final output 用来进行输出， 输出和attention weight 是不一样的\n",
    "        ## block1 和 block2 是自注意力和交叉注意力 的参数，所以是不是模型内部的参数指的就是 block1 和 block2 的参数\n",
    "        ## 遍历 self.dec_layers 列表中的每一层\n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "            x, block1, block2 = dec_layer(x, enc_output, src_mask, tgt_mask)  # [bs, seq_len, emb] [bs, seq_len, emb], [bs, heads, 1, seq_len], [bs, 1, seq_len, seq_len]\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "        \n",
    "        # 最后通过全连接层映射到词汇表大小\n",
    "        final_output = self.fc_out(x)  # (batch_size, target_seq_len, vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights"
   ],
   "id": "bbc35dc2c34025f0",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T12:46:25.468218Z",
     "start_time": "2024-12-10T12:46:25.459193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_decoder_layers, dim_feedforward, max_seq_length, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # 加载分词器\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"./tokenizer_files/\",  # 替换为你的实际路径\n",
    "            trust_remote_code=True,\n",
    "            truncation_side='right',\n",
    "            padding_side='right'\n",
    "        )   \n",
    "        # 解码器层\n",
    "        self.decoder = Decoder(num_decoder_layers, d_model, num_heads, dim_feedforward, dropout, max_seq_length)\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src_input_ids, encoder_outputs, src_mask, tgt_mask):\n",
    "        \n",
    "        # 模拟编码器输出（随机初始化） !!!!!!!!!!!!!!!!!!!! \n",
    "        encoder_outputs = torch.randn(src_input_ids.size(0), src_input_ids.size(1), d_model)  # [batch_size, seq_len_src, d_model]\n",
    "        \n",
    "        # 初始化模型\n",
    "        # encoder = Encoder(num_layers, d_model, num_heads, d_ff, dropout)  # 假设有 Encoder 类\n",
    "        decoder = Decoder(num_layers, d_model, num_heads, d_ff, dropout, tokenizer.vocab_size)\n",
    "        \n",
    "        # 编码器输出\n",
    "        # encoder_outputs = encoder(src_input_ids, src_mask)  # 假设返回形状为 [batch_size, seq_len, d_model]\n",
    "        # 解码器输出 attn_weights 没有做处理\n",
    "        output, attn_weights = decoder(src_input_ids, encoder_outputs, src_mask, tgt_mask)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "    def generate(self, start_token, max_len, src_input_ids, src_mask, tgt_mask):\n",
    "        with torch.no_grad():\n",
    "            # 确保 start_token 是整数类型，并初始化生成序列\n",
    "            if not isinstance(start_token, int):\n",
    "                raise ValueError(\"start_token must be an integer representing a valid token ID.\")\n",
    "            generated_sequence = [start_token]\n",
    "    \n",
    "            for i in range(max_len - 1):  # 减一因为已经包含了一个 start_token\n",
    "                # 构造当前的目标序列张量，确保所有元素都是整数类型\n",
    "                tgt_tensor = torch.tensor(generated_sequence, dtype=torch.long).unsqueeze(0).to(next(self.parameters()).device)\n",
    "                \n",
    "                # 检查生成的 tgt_tensor 是否包含有效的 token 索引\n",
    "                if tgt_tensor.max() >= self.embedding.num_embeddings:\n",
    "                    raise ValueError(f\"Generated token index {tgt_tensor.max().item()} exceeds embedding size {self.embedding.num_embeddings}.\")\n",
    "                \n",
    "             \n",
    "                # 通过 forward 函数获取解码器输出\n",
    "                output = self(src_input_ids, tgt_tensor, src_mask, tgt_mask)  # 使用 self(...) 而不是 self.forward(...)\n",
    "                print(output.size())\n",
    "                # 从输出中选择概率最大的 token ID，并确保它是整数类型\n",
    "                next_token = int(output.argmax(dim=-1)[:, -1].item())\n",
    "                \n",
    "                # 将下一个 token 添加到生成序列中\n",
    "                generated_sequence.append(next_token)\n",
    "    \n",
    "                # 如果遇到了结束标记，则停止生成\n",
    "                if next_token == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "            \n",
    "            return generated_sequence\n",
    "\n",
    "    @classmethod\n",
    "    def generate_square_subsequent_mask(cls, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ],
   "id": "3062be4fff00e195",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:59:36.801647Z",
     "start_time": "2024-12-10T13:59:36.237192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test Transformer's forward function\n",
    "def simple_test_transformer_model():\n",
    "    import torch\n",
    "\n",
    "    # 参数设置\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    d_model = 1536\n",
    "    nhead = 4\n",
    "    num_decoder_layers = 2\n",
    "    dim_feedforward = 256\n",
    "    max_seq_length = 5\n",
    "    dropout = 0.1\n",
    "\n",
    "    # 初始化模型\n",
    "    model = TransformerModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    # batch_size = 2\n",
    "    source_texts = [\"Translate this sentence.\", \"Another example sentence, final example sentence.\"]\n",
    "    \n",
    "    # 关注 src_input_ids, src_mask 和 tgt_mask\n",
    "    # 分词\n",
    "    tokenized_source = tokenizer(source_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    src_input_ids = tokenized_source[\"input_ids\"]  # [batch, seq_len]\n",
    "    src_attention_mask = tokenized_source[\"attention_mask\"]\n",
    "    \n",
    "    ## 目标序列掩码（decoder self-attention） causal mask 不看当前位置之后\n",
    "    seq_len = src_input_ids.size(1)\n",
    "    # 下三角包括对角线全为1\n",
    "    tgt_mask = torch.torch.tril(torch.ones(seq_len, seq_len))\n",
    "    # tgt_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)  # [seq_len, seq_len]\n",
    "    tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(1).to(src_input_ids.device)  # [1, 1, seq_len, seq_len]\n",
    "    tgt_mask = tgt_mask.expand(batch_size, -1, -1, -1) # [batch_size, 1, seq_len, seq_len]\n",
    "    \n",
    "    ## padding mask 不看padding\n",
    "    src_mask = src_attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "    src_mask = src_mask.expand(-1, num_heads, -1, -1)  # [batch_size, num_heads, 1, seq_len]\n",
    "    # \n",
    "    # print(f\"src_input_ids:{src_input_ids}\")\n",
    "    # print(f\"src_input_ids_size: {src_input_ids.size()}\")\n",
    "    # print(f\"tgt_mask: {tgt_mask}\")\n",
    "    # print(f\"tgt_mask_size: {tgt_mask.size()}\")\n",
    "    # print(f\"src_mask: {src_mask}\")\n",
    "    # print(f\"src_mask_size:{src_mask.size()}\")\n",
    "    # 前向传播测试 测试 Transformer 的 forward 函数成功 \n",
    "    # 第一个参数是decoder的input。第二个参数不重要。第三个参数是防止padding参与运算，第四个参数是自回归掩码\n",
    "    output = model(src_input_ids, src_input_ids, src_mask, tgt_mask)\n",
    "    print(\"Forward pass output shape:\", output.shape)  # 应为 (batch_size, seq_length, vocab_size)\n",
    "\n",
    "# 运行测试函数\n",
    "simple_test_transformer_model()\n"
   ],
   "id": "d80fd1294ba5fce0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "q.size: torch.Size([2, 10, 512])\n",
      "k.size: torch.Size([2, 10, 512])\n",
      "v.size: torch.Size([2, 10, 512])\n",
      "new_q.size: torch.Size([2, 8, 10, 64])\n",
      "new_k.size: torch.Size([2, 8, 10, 64])\n",
      "new_v.size: torch.Size([2, 8, 10, 64])\n",
      "Forward pass output shape: torch.Size([2, 10, 30522])\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T04:36:17.467882Z",
     "start_time": "2024-12-08T04:36:16.753732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 推理的时候进行单条单条的推理\n",
    "def simple_test_inference_transformer_model():\n",
    "    import torch\n",
    "\n",
    "    # 参数设置\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    d_model = 1536\n",
    "    nhead = 4\n",
    "    num_decoder_layers = 2\n",
    "    dim_feedforward = 256\n",
    "    max_seq_length = 5\n",
    "    dropout = 0.1\n",
    "\n",
    "    # 初始化模型\n",
    "    model = TransformerModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    source_texts = [\"Translate this sentence.\"]\n",
    "    target_texts = [\"Translate this sentence.\"]\n",
    "    \n",
    "    # 准备两个tokenizer 和 两个掩码\n",
    "    # 分词\n",
    "    tokenized_source = tokenizer(source_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    tokenized_target = tokenizer(target_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    src_input_ids = tokenized_source[\"input_ids\"]  # [batch, seq_len]\n",
    "    tgt_input_ids = tokenized_target[\"input_ids\"]  # [batch, seq_len]\n",
    "    \n",
    "    src_attention_mask = tokenized_source[\"attention_mask\"]\n",
    "    tgt_attention_mask = tokenized_target[\"attention_mask\"]\n",
    "    \n",
    "    ## 生成掩码 padding mask 不看padding\n",
    "    src_mask = src_attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "    src_mask = src_mask.expand(-1, num_heads, -1, -1)  # [batch_size, num_heads, 1, seq_len]\n",
    "    \n",
    "    ## 目标序列掩码（decoder self-attention） causal mask 不看当前位置之后\n",
    "    seq_len = tgt_input_ids.size(1)\n",
    "    # 上三角对角矩阵 mask 矩阵来说不是 0 就是 1\n",
    "    tgt_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)  # [seq_len, seq_len]\n",
    "    tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(1).to(tgt_input_ids.device)  # [1, 1, seq_len, seq_len]\n",
    "    # tgt_mask = tgt_mask.expand(batch_size, -1, -1, -1)\n",
    "    \n",
    "    print(src_input_ids.size())\n",
    "    print(tgt_input_ids.size())\n",
    "    print(src_mask.size())\n",
    "    print(tgt_mask.size())\n",
    "    # 前向传播测试 测试 Transformer 的 forward 函数成功\n",
    "    output = model(src_input_ids, tgt_input_ids, src_mask, tgt_mask)\n",
    "    print(\"Forward pass output shape:\", output.shape)  # 应为 (batch_size, seq_length, vocab_size)\n",
    "\n",
    "    \n",
    "    # 设置起始 token\n",
    "    start_token = tokenizer.cls_token_id if tokenizer.cls_token_id is not None else 1  # 假设 cls_token 作为起始 token\n",
    "\n",
    "    # 调用生成函数\n",
    "    print(\"Generation test:\")\n",
    "    generated_seq = model.generate(start_token=start_token, max_len=max_seq_length, \n",
    "                                   src_input_ids=src_input_ids, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "    # 检查生成的序列\n",
    "    print(\"Generated token sequence:\", generated_seq)\n",
    "\n",
    "    # 反编码为文本\n",
    "    generated_text = tokenizer.decode(generated_seq, skip_special_tokens=True)\n",
    "    print(\"Generated text:\", generated_text)\n",
    "    \n",
    "    \n",
    "    # # 模拟输入数据\n",
    "    # tgt = torch.randint(0, vocab_size, (2, max_seq_length))  # 批大小2，序列长度为max_seq_length\n",
    "    # enc_output = torch.rand(2, max_seq_length, d_model)  # 假设编码器的输出\n",
    "    # src_mask = None  # 示例中不使用具体的mask\n",
    "    # tgt_mask = TransformerModel.generate_square_subsequent_mask(max_seq_length)\n",
    "    # \n",
    "\n",
    "    # 生成测试\n",
    "    ## 设置起始 token\n",
    "    start_token = 1\n",
    "\n",
    "    # 假设编码器的输出\n",
    "    enc_output = torch.rand(2, max_seq_length, d_model)\n",
    "    print(encoder_outputs.size())\n",
    "\n",
    "    # 调用生成函数\n",
    "    print(1)\n",
    "    ## 从 start_token 开始生成内容 start_token: max_seq_length: enc_output: \n",
    "    # 有没有必要设置 src_mask, tgt_mask？？？？？\n",
    "    generated_seq = model.generate(max_seq_length, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "    # 检查生成的序列\n",
    "    print(\"Generated token sequence:\", generated_seq)\n",
    "    \n",
    "    print(4)\n",
    "    # 反编码为文本\n",
    "    generated_text = model.tokenizer.decode(generated_seq, skip_special_tokens=True)\n",
    "    print(\"Generated text:\", generated_text)\n",
    "\n",
    "# 运行测试函数\n",
    "simple_test_transformer_model()\n"
   ],
   "id": "564499aef2d2abc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n",
      "torch.Size([2, 6])\n",
      "torch.Size([2, 8, 1, 6])\n",
      "torch.Size([2, 1, 6, 6])\n",
      "Forward pass output shape: torch.Size([2, 6, 30522])\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T13:15:11.243852Z",
     "start_time": "2024-12-07T13:15:11.228921Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.additional_special_tokens[0])",
   "id": "7ef0199b84408a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>\n"
     ]
    }
   ],
   "execution_count": 264
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T11:52:25.031977Z",
     "start_time": "2024-12-07T11:52:25.028069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        tgt_input = batch['input'].to(device)\n",
    "        tgt_output = batch['output'].to(device)\n",
    "        enc_output = batch.get('enc_output', None)  # 如果有编码器输出\n",
    "        src_mask = batch.get('src_mask', None)\n",
    "        tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(tgt_input, enc_output, src_mask, tgt_mask)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt_output.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ],
   "id": "7803199314febfb",
   "outputs": [],
   "execution_count": 214
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T11:59:35.990732Z",
     "start_time": "2024-12-07T11:59:35.987327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_mock_data(batch_size, seq_length, vocab_size):\n",
    "    # 创建随机输入序列，保证 token ID 在 [0, vocab_size - 1] 范围内\n",
    "    input_sequences = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "    output_sequences = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "    \n",
    "    # 其他部分保持不变...\n",
    "    enc_output = torch.randn(batch_size, seq_length, d_model)  # 示例编码器输出\n",
    "    src_mask = torch.ones(batch_size, 1, seq_length)           # 示例源序列掩码\n",
    "    \n",
    "    return {\n",
    "        'input': input_sequences,\n",
    "        'output': output_sequences,\n",
    "        'enc_output': enc_output,\n",
    "        'src_mask': src_mask\n",
    "    }"
   ],
   "id": "aa3077370ee31f56",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T12:03:08.892378Z",
     "start_time": "2024-12-07T12:03:07.939744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_transformer_model():\n",
    "\n",
    "    seq_length = 2\n",
    "    # 初始化模型、损失函数和优化器\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TransformerModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        nhead=n_heads,\n",
    "        num_decoder_layers=num_layers,\n",
    "        dim_feedforward=d_ff,\n",
    "        max_seq_length=seq_len\n",
    "    ).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=model.tokenizer.pad_token_id)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # 创建模拟数据加载器\n",
    "    dataloader = [create_mock_data(batch_size, seq_length, vocab_size) for _ in range(2)]  # 创建两个批次的数据\n",
    "\n",
    "    # 训练模型\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1):  # 只进行一轮训练作为示例\n",
    "        train_loss = train(model, dataloader, criterion, optimizer, device)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {train_loss}\")\n",
    "\n",
    "    # 进行推理\n",
    "    print(\"Starting inference...\")\n",
    "    start_token = model.tokenizer.bos_token_id  # 使用开始标记的ID\n",
    "    max_len = 10\n",
    "    enc_output = torch.randn(1, seq_length, d_model).to(device)  # 示例编码器输出，注意d_model应与模型一致\n",
    "    src_mask = torch.ones(1, 1, seq_length).to(device)  # 示例源序列掩码\n",
    "    \n",
    "    # 确保传入的start_token是整数类型\n",
    "    generated_sequence_ids = infer(model, start_token, max_len, enc_output, src_mask, device)\n",
    "    \n",
    "    # 将生成的token ID转换为文本（假设有一个对应的分词器方法）\n",
    "    generated_text = model.tokenizer.decode(generated_sequence_ids, skip_special_tokens=True)\n",
    "    print(\"Generated text:\", generated_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_transformer_model()"
   ],
   "id": "a48ae92927914d26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[224], line 41\u001B[0m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerated text:\u001B[39m\u001B[38;5;124m\"\u001B[39m, generated_text)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 41\u001B[0m     \u001B[43mtest_transformer_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[224], line 23\u001B[0m, in \u001B[0;36mtest_transformer_model\u001B[1;34m()\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting training...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m):  \u001B[38;5;66;03m# 只进行一轮训练作为示例\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# 进行推理\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[214], line 13\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dataloader, criterion, optimizer, device)\u001B[0m\n\u001B[0;32m     10\u001B[0m tgt_mask \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate_square_subsequent_mask(tgt_input\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 13\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, output\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)), tgt_output\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     15\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[213], line 30\u001B[0m, in \u001B[0;36mTransformerModel.forward\u001B[1;34m(self, tgt, enc_output, src_mask, tgt_mask)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     28\u001B[0m     tgt_tokens \u001B[38;5;241m=\u001B[39m tgt  \u001B[38;5;66;03m# 假设此时 tgt 是一个张量\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc_out(output)\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[183], line 27\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[1;34m(self, x, enc_output, src_mask, tgt_mask)\u001B[0m\n\u001B[0;32m     24\u001B[0m seq_len \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# 添加嵌入和位置编码\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding\u001B[38;5;241m.\u001B[39membedding_dim)\n\u001B[0;32m     28\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_encoding(x)\n\u001B[0;32m     29\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2545\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2546\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2547\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2548\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2549\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2550\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2551\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "execution_count": 224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T08:07:44.978134Z",
     "start_time": "2024-12-07T08:07:44.971689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 三种掩码方式：padding mask, causal mask, combined mask\n",
    "import torch\n",
    "\n",
    "# 假设输入数据\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "num_heads = 8\n",
    "\n",
    "# 假设填充 token ID 是 0\n",
    "input_ids = torch.tensor([\n",
    "    [1, 2, 3, 0, 0],  # 第一个序列，最后两个是填充\n",
    "    [4, 5, 6, 7, 0]   # 第二个序列，最后一个是填充\n",
    "])\n",
    "# 1. 构造 Padding Mask # padding_mask: 1 表示有效位置，0 表示填充位置\n",
    "padding_mask = (input_ids != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "# 2. 构造 Causal Mask\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()  # (seq_len, seq_len)\n",
    "causal_mask = causal_mask.unsqueeze(0).expand(batch_size, -1, -1)  # (batch_size, seq_len, seq_len)\n",
    "print(causal_mask)\n",
    "\n",
    "print(padding_mask)\n",
    "# 3. Combine Masks\n",
    "# 使用或 运算符 \n",
    "combined_mask = causal_mask | ~padding_mask.squeeze(1).expand(-1, seq_len, -1)  # (batch_size, seq_len, seq_len)\n",
    "print(combined_mask)"
   ],
   "id": "8caee1453995dad4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False, False]]])\n",
      "tensor([[[[ True,  True,  True, False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True,  True,  True, False]]]])\n",
      "tensor([[[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [False, False, False,  True,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True]]])\n"
     ]
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "31d40707b0197302"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![pe](./decoder_img/position%20embedding.png)",
   "id": "610f1bba7b434253"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://blog.csdn.net/Q52099999/article/details/136180399",
   "id": "979c7353a9ccfe36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:55:08.569278Z",
     "start_time": "2024-12-05T11:55:08.565261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ],
   "id": "9de7a3e3e513996a",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:55:10.224165Z",
     "start_time": "2024-12-05T11:55:10.219573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].detach()\n",
    "        return self.dropout(x)"
   ],
   "id": "b470d9ead69089ca",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:55:13.527218Z",
     "start_time": "2024-12-05T11:55:13.428718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test PositionalEncoding\n",
    "input=torch.ones(4,10,512)\n",
    "positional_encoding=PositionalEncoding(512,0.1)\n",
    "output=positional_encoding(input)\n",
    "print(output.shape)     #torch.Size([4, 10, 512])\n",
    "print(output)\n"
   ],
   "id": "cedf1b2001a83b4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 512])\n",
      "tensor([[[1.1111, 2.2222, 1.1111,  ..., 2.2222, 1.1111, 2.2222],\n",
      "         [2.0461, 1.7114, 2.0243,  ..., 2.2222, 1.1112, 2.2222],\n",
      "         [2.1214, 0.6487, 2.1516,  ..., 2.2222, 1.1113, 2.2222],\n",
      "         ...,\n",
      "         [1.8411, 0.0000, 1.6138,  ..., 2.2222, 1.1119, 2.2222],\n",
      "         [2.2104, 0.9494, 2.2119,  ..., 0.0000, 1.1120, 2.2222],\n",
      "         [1.5690, 0.0987, 1.8626,  ..., 2.2222, 1.1121, 2.2222]],\n",
      "\n",
      "        [[1.1111, 2.2222, 1.1111,  ..., 2.2222, 1.1111, 0.0000],\n",
      "         [2.0461, 1.7114, 2.0243,  ..., 2.2222, 1.1112, 2.2222],\n",
      "         [2.1214, 0.6487, 2.1516,  ..., 0.0000, 1.1113, 2.2222],\n",
      "         ...,\n",
      "         [1.8411, 1.9488, 0.0000,  ..., 2.2222, 1.1119, 0.0000],\n",
      "         [2.2104, 0.9494, 2.2119,  ..., 2.2222, 1.1120, 2.2222],\n",
      "         [1.5690, 0.0987, 1.8626,  ..., 2.2222, 1.1121, 2.2222]],\n",
      "\n",
      "        [[1.1111, 2.2222, 1.1111,  ..., 2.2222, 1.1111, 2.2222],\n",
      "         [2.0461, 1.7114, 2.0243,  ..., 2.2222, 0.0000, 2.2222],\n",
      "         [2.1214, 0.6487, 2.1516,  ..., 2.2222, 1.1113, 2.2222],\n",
      "         ...,\n",
      "         [1.8411, 1.9488, 1.6138,  ..., 2.2222, 0.0000, 2.2222],\n",
      "         [2.2104, 0.9494, 2.2119,  ..., 2.2222, 1.1120, 2.2222],\n",
      "         [1.5690, 0.0987, 1.8626,  ..., 2.2222, 1.1121, 2.2222]],\n",
      "\n",
      "        [[1.1111, 2.2222, 1.1111,  ..., 2.2222, 1.1111, 2.2222],\n",
      "         [2.0461, 1.7114, 2.0243,  ..., 0.0000, 1.1112, 2.2222],\n",
      "         [2.1214, 0.6487, 2.1516,  ..., 2.2222, 1.1113, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 1.9488, 1.6138,  ..., 2.2222, 1.1119, 2.2222],\n",
      "         [2.2104, 0.0000, 2.2119,  ..., 2.2222, 0.0000, 0.0000],\n",
      "         [1.5690, 0.0987, 1.8626,  ..., 0.0000, 1.1121, 2.2222]]])\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![sdqa](./decoder_img/sdpa.png)\n",
    "![multi-head-attention](./decoder_img/multi-head%20attention.png)\n",
    "![formula](./decoder_img/formula_multi-head%20attention.png)"
   ],
   "id": "3602be56008e5910"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://hwcoder.top/Manual-Coding-1",
   "id": "d7059d18562122b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:55:17.007510Z",
     "start_time": "2024-12-05T11:55:17.003312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## SDPA\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query, key, value, attention_mask=None):\n",
    "        # query, key, value 形状: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        # key.transpose(-1, -2) 将最后两个维度进行转置，以进行点积\n",
    "        # attention_scores 形状: (batch_size, seq_len, seq_len)\n",
    "        d_k = query.size(-1)  # 获取 hidden_size\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        \n",
    "        # 添加注意力掩码（seq_len, seq_len），掩码位置（1）的值为负无穷\n",
    "        if attention_mask is not None:\n",
    "            attention_scores += attention_mask * -1e9\n",
    "                \n",
    "        # 对注意力分数进行归一化，得到注意力概率\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # 计算注意力输出，通过注意力概率加权值\n",
    "        attention_output = torch.matmul(attention_probs, value)  # (batch_size, num_heads, seq_len, hidden_size)\n",
    "        \n",
    "        return attention_output"
   ],
   "id": "679af9d551ab90ae",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:55:18.574546Z",
     "start_time": "2024-12-05T11:55:17.761317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_attn():\n",
    "    \n",
    "    query = torch.randn(batch_size, seq_len, hidden_size)  # (batch_size, seq_len, hidden_size)\n",
    "    key = torch.randn(batch_size, seq_len, hidden_size)    # (batch_size, seq_len, hidden_size)\n",
    "    value = torch.randn(batch_size, seq_len, hidden_size)  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "    sdpa = ScaledDotProductAttention()\n",
    "    output = sdpa(query, key, value)\n",
    "    \n",
    "    print(\"Query shape:\", query.shape)\n",
    "    print(\"Key shape:\", key.shape)\n",
    "    print(\"Value shape:\", value.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\ttest_attn()"
   ],
   "id": "7e47cfce104c27a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([128, 512, 1024])\n",
      "Key shape: torch.Size([128, 512, 1024])\n",
      "Value shape: torch.Size([128, 512, 1024])\n",
      "Output shape: torch.Size([128, 512, 1024])\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:55:25.141316Z",
     "start_time": "2024-12-05T11:55:25.135219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 编码 MHA\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads  # 每个头的维度，二者必须整除\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 初始化 Q、K、V 的投影矩阵，将输入词向量线性变换为 Q、K、V，维度保持一致\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size) \n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # 输出线性层，将拼接后的多头注意力输出变换为所需的输出维度，这里维度保持一致\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Dropout 层\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, hidden_state, attention_mask=None):\n",
    "        # hidden_state 形状: (batch_size, seq_len, hidden_size)\n",
    "        batch_size = hidden_state.size(0)  # 获取批量大小\n",
    "\n",
    "        # 计算 Q、K、V，线性变换\n",
    "        query = self.q_linear(hidden_state)  # (batch_size, seq_len, hidden_size)\n",
    "        key = self.k_linear(hidden_state)    # (batch_size, seq_len, hidden_size)\n",
    "        value = self.v_linear(hidden_state)  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # 分割多头，将每个头的维度拆分出来\n",
    "        query = self.split_head(query)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        key = self.split_head(key)      # (batch_size, num_heads, seq_len, head_dim)\n",
    "        value = self.split_head(value)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # 计算注意力分数，使用缩放点积注意力机制\n",
    "        # attention_scores 形状: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        \n",
    "        # 添加注意力掩码（seq_len, seq_len），掩码位置（1）的值为负无穷\n",
    "        if attention_mask is not None:\n",
    "            attention_scores += attention_mask * -1e9\n",
    "        \n",
    "        # 对注意力分数进行归一化，得到注意力概率\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # 计算注意力输出，通过注意力概率加权值\n",
    "        output = torch.matmul(attention_probs, value)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # 对多头注意力输出进行拼接\n",
    "        # output.transpose(1, 2) 将 num_heads 和 seq_len 维度转置\n",
    "        # 将形状调整为 (batch_size, seq_len, hidden_size)\n",
    "        output = output.transpose(1, 2).reshape(batch_size, -1, self.head_dim * self.num_heads)\n",
    "        \n",
    "        # 通过线性层将拼接后的输出变换为所需的输出维度\n",
    "        output = self.o_linear(output)  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def split_head(self, x):\n",
    "        batch_size = x.size(0)  # 获取批量大小\n",
    "        # x 形状: (batch_size, seq_len, hidden_size)\n",
    "        # 将 hidden_size 分割为 num_heads 和 head_dim\n",
    "        return x.reshape(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # 返回形状: (batch_size, num_heads, seq_len, head_dim)"
   ],
   "id": "6ffdd11a0a246c9",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:55:28.280242Z",
     "start_time": "2024-12-05T11:55:27.000989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_MHA():\n",
    "    \n",
    "    # 随机生成输入数据\n",
    "    hidden_state = torch.randn(batch_size, seq_len, hidden_size)  # (batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    # 创建多头注意力模块\n",
    "    mha = MultiHeadAttention(hidden_size, num_heads)\n",
    "    \n",
    "    # 计算多头注意力输出\n",
    "    output = mha(hidden_state)\n",
    "    \n",
    "    print(\"Input shape:\", hidden_state.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(hidden_state - output) # 说明输入和输出是有差距的\n",
    "if __name__ == \"__main__\":\n",
    "\ttest_MHA()"
   ],
   "id": "815afbae8c2f5f25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 512, 1024])\n",
      "Output shape: torch.Size([128, 512, 1024])\n",
      "tensor([[[-1.6193, -0.0891, -0.4542,  ...,  0.4297, -0.8153,  0.3470],\n",
      "         [-2.7809, -0.7091, -0.4975,  ...,  0.7200,  0.1808,  0.2124],\n",
      "         [ 1.3735, -0.8517,  0.3719,  ..., -1.7809,  1.1514,  0.3892],\n",
      "         ...,\n",
      "         [-1.3494,  0.2579, -1.1982,  ...,  0.5016,  2.3327,  0.4530],\n",
      "         [-1.3341,  0.5193,  1.6964,  ..., -1.6268,  0.1739,  0.8901],\n",
      "         [ 0.3217,  0.3426,  0.6627,  ..., -0.6213,  0.6771,  0.6266]],\n",
      "\n",
      "        [[ 1.1545, -1.5229, -0.1207,  ..., -0.8561,  0.7075,  0.1770],\n",
      "         [ 1.0253,  1.1284,  0.5222,  ...,  0.0522, -0.6267,  0.6013],\n",
      "         [-0.5846, -0.4513, -0.8590,  ..., -0.3691, -0.7737, -1.9918],\n",
      "         ...,\n",
      "         [ 0.8158, -2.1588,  0.0743,  ..., -1.1199,  0.2521, -0.0692],\n",
      "         [-1.1471, -1.4210,  0.8020,  ..., -1.3011,  0.8843,  0.4221],\n",
      "         [-0.6515,  2.2974, -0.8135,  ...,  0.4630,  0.1544, -1.3763]],\n",
      "\n",
      "        [[ 0.2843,  1.0236, -0.3731,  ..., -0.5097, -0.4753, -0.5715],\n",
      "         [ 0.0188, -0.9343, -0.3832,  ...,  1.0608, -0.4404,  0.7519],\n",
      "         [ 0.5476,  0.0682, -0.8919,  ...,  0.0984,  0.9631, -0.4572],\n",
      "         ...,\n",
      "         [ 1.3778, -0.0812, -1.0221,  ..., -0.4697,  0.4809, -0.0817],\n",
      "         [-2.3030,  0.9065, -1.0501,  ...,  0.4671,  0.1522, -0.1981],\n",
      "         [ 1.6109, -1.2598,  1.6280,  ..., -0.6753, -1.1916, -2.7965]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.4379, -1.1008, -1.1545,  ...,  0.0242, -0.1280,  0.7209],\n",
      "         [ 1.6011,  0.2005, -0.0584,  ..., -0.5876,  0.0880, -0.1024],\n",
      "         [ 2.6691,  1.2385, -1.8045,  ...,  0.0717, -0.4409, -0.4761],\n",
      "         ...,\n",
      "         [-1.0377,  0.2031,  1.0571,  ...,  1.3792,  0.5915,  0.3300],\n",
      "         [ 1.4652,  0.0272, -1.6066,  ..., -0.3094,  0.5693,  0.1360],\n",
      "         [-0.5465, -0.8371, -1.2481,  ...,  0.4204, -0.0475,  0.7642]],\n",
      "\n",
      "        [[-1.3492,  2.1008, -0.3787,  ...,  1.4996,  1.3930,  0.3562],\n",
      "         [ 0.5657, -2.2790,  0.6837,  ..., -1.1656, -0.2849,  0.0225],\n",
      "         [-0.1919, -0.8885,  1.1016,  ...,  0.6066, -0.3320, -0.9893],\n",
      "         ...,\n",
      "         [-0.4428,  0.7200,  1.4506,  ...,  0.6009,  1.4740,  0.6439],\n",
      "         [-0.0722, -1.1830, -1.6275,  ..., -0.0821,  1.1167, -0.4758],\n",
      "         [ 0.3042,  1.5113,  0.8709,  ..., -0.0090,  1.8559,  0.6162]],\n",
      "\n",
      "        [[ 1.9808, -1.5842, -0.7904,  ...,  0.1248, -1.6230,  0.4616],\n",
      "         [-0.9888,  1.7927,  0.5798,  ..., -0.0644, -0.4382,  0.1286],\n",
      "         [ 0.5490, -1.3033, -0.4666,  ...,  0.1139,  0.3241, -1.2824],\n",
      "         ...,\n",
      "         [-1.1178, -0.7477,  0.5030,  ...,  0.5730, -0.4192, -0.1885],\n",
      "         [-0.9584,  0.8139,  0.8563,  ...,  1.2079,  0.4617, -0.0185],\n",
      "         [ 0.3121, -0.1162, -0.0332,  ..., -0.6238,  0.0852, -0.7640]]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![FFN](./decoder_img/FFN.png)",
   "id": "86294e5e6514d032"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T02:03:21.316948Z",
     "start_time": "2024-12-08T02:03:21.313134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n"
   ],
   "id": "7fe590aac806518c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T11:55:39.496527Z",
     "start_time": "2024-12-05T11:55:38.999437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 测试代码\n",
    "def test_positionwise_feedforward():\n",
    "    # 创建一个随机输入张量，形状为 [batch_size, seq_len, d_model]\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # 实例化 PositionWiseFeedForward 模块\n",
    "    ff = PositionWiseFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "\n",
    "    # 打印输入张量的形状\n",
    "    print(\"Input shape:\", x.shape)\n",
    "\n",
    "    # 前向传播\n",
    "    output = ff(x)\n",
    "\n",
    "    # 打印输出张量的形状\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "# 运行测试\n",
    "test_positionwise_feedforward()"
   ],
   "id": "6e720af0e2ffc6e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 512, 512])\n",
      "Output shape: torch.Size([128, 512, 512])\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### my purpose\n",
    "\n",
    "# load tokenizer and decoder/encoder/encoder-decoder model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizer\n",
    "inputs = tokenizer(\"Hello, my dog is cute.\", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "# use decoder to generate\n",
    "generated_ids = model.generate(**inputs)\n",
    "# decoder to text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)\n",
    "\n",
    "\n",
    "# use encoder to embedding\n",
    "outputs = model(**inputs)\n",
    "# get the last layer's hidden state\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(\"Embeddings shape:\", last_hidden_states.shape)"
   ],
   "id": "7c935932973bf105"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "next question:\n",
    "1. how to convert input text to tokenizer ID using tokenizer model?\n",
    "2. how to transfer the tokenizer ID to model's input embedding?\n",
    "3. how to use embedding to generate the final answer?"
   ],
   "id": "bae66e8c07a2a878"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:50:26.668045Z",
     "start_time": "2024-12-05T12:50:26.660722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # 定义线性变换层\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "    def split_head(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, head_dim).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # 线性变换\n",
    "        q = self.q_linear(q)  # (batch_size, seq_len_q, hidden_size)\n",
    "        k = self.k_linear(k)  # (batch_size, seq_len_k, hidden_size)\n",
    "        v = self.v_linear(v)  # (batch_size, seq_len_v, hidden_size)\n",
    "\n",
    "        # 分割头部\n",
    "        q = self.split_head(q, batch_size)  # (batch_size, num_heads, seq_len_q, head_dim)\n",
    "        k = self.split_head(k, batch_size)  # (batch_size, num_heads, seq_len_k, head_dim)\n",
    "        v = self.split_head(v, batch_size)  # (batch_size, num_heads, seq_len_v, head_dim)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # 应用注意力权重到值向量\n",
    "        output = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, head_dim)\n",
    "\n",
    "        # 合并头部\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)  # (batch_size, seq_len_q, hidden_size)\n",
    "\n",
    "        # 最后一个线性变换\n",
    "        output = self.o_linear(output)  # (batch_size, seq_len_q, hidden_size)\n",
    "\n",
    "        return output, attention_weights"
   ],
   "id": "eb367e166008555a",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![decoder](./decoder_img/decoder_code.png)",
   "id": "170a6f8025957721"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T13:08:34.582610Z",
     "start_time": "2024-12-05T13:08:34.576503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, vocab_size):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # 自注意力层（Self-Attention）\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # 编码器-解码器注意力层（Cross-Attention）\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # 前馈网络（Feed-Forward）\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        \n",
    "        # LayerNorm 层（Layer Normalization）\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout 层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # 将输入索引转换为嵌入向量，并调整尺度\n",
    "       \n",
    "        # 自注意力：输入为解码器自身的输入（x）\n",
    "        attn_output, block1 = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # 残差连接和 LayerNorm\n",
    "        \n",
    "        # 交叉注意力：输入为编码器输出（enc_output）和解码器输入（x）\n",
    "        attn_output, block2 = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))  # 残差连接和 LayerNorm\n",
    "        \n",
    "        # 前馈网络（Feed-Forward）\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))  # 残差连接和 LayerNorm\n",
    "        \n",
    "        return x, block1, block2"
   ],
   "id": "677580a981b371d1",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T13:08:46.137926Z",
     "start_time": "2024-12-05T13:08:46.133212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout, vocab_size, max_len=5000):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 位置编码\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
    "        \n",
    "        # 解码器层列表\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout, vocab_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attention_weights = {}\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # 添加嵌入和位置编码\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "            x, block1, block2 = dec_layer(x, enc_output, src_mask, tgt_mask)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "        \n",
    "        # 最后通过全连接层映射到词汇表大小\n",
    "        final_output = self.fc_out(x)  # (batch_size, target_seq_len, vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights"
   ],
   "id": "317e5f697bc3e0ef",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T13:31:25.827966Z",
     "start_time": "2024-12-05T13:31:25.660648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 示例参数设置\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "vocab_size = 10000\n",
    "num_layers = 6\n",
    "\n",
    "# 创建随机输入\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))  # 解码器输入，类型为 Long\n",
    "enc_output = torch.randn(batch_size, seq_len, d_model)   # 编码器输出，类型为 Float\n",
    "src_mask = torch.ones(batch_size, 1, 1, seq_len)         # 源序列的 mask，类型为 Float\n",
    "tgt_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)  # 目标序列的 mask\n",
    "tgt_mask = tgt_mask.expand(batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "print(enc_output.size())\n",
    "print(src_mask.size())\n",
    "print(tgt_mask.size())\n",
    "\n",
    "# 初始化 Decoder\n",
    "decoder = Decoder(num_layers, d_model, num_heads, d_ff, dropout, vocab_size)\n",
    "\n",
    "# 获取解码器的输出\n",
    "output, attn_weights = decoder(x, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "# 输出的形状应该是 [batch_size, seq_len, vocab_size]\n",
    "print(output.shape)  # 应该是 (32, 10, 10000)"
   ],
   "id": "d33a948b5d801f0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 512])\n",
      "torch.Size([32, 1, 1, 10])\n",
      "torch.Size([32, 8, 10, 10])\n",
      "torch.Size([32, 10, 10000])\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T13:34:33.346613Z",
     "start_time": "2024-12-05T13:34:33.343505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 输入数据\n",
    "# 示例文本\n",
    "source_texts = [\"Translate this sentence.\", \"Another example sentence.\"]\n",
    "target_texts = [\"Translate this sentence.\", \"Another example sentence.\"]"
   ],
   "id": "bec984a02f3da87a",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T13:34:34.089400Z",
     "start_time": "2024-12-05T13:34:33.926027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## step 1: tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 本地分词器目录路径\n",
    "local_tokenizer_dir = \"./tokenizer_files/\"  # 替换为你的实际路径\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_tokenizer_dir,\n",
    "    trust_remote_code=True,  # 如果使用的分词器有自定义代码，需要启用此选项\n",
    "    truncation_side='right', # 设置分词器的截断侧\n",
    "    padding_side='right'     # 设置分词器的填充侧\n",
    ")\n",
    "\n",
    "# 分词\n",
    "tokenized_output = tokenizer(\n",
    "    source_texts, \n",
    "    return_tensors=\"pt\",  # 返回 PyTorch 张量\n",
    "    padding=True,         # 启用填充\n",
    "    truncation=True       # 启用截断\n",
    ")\n",
    "\n",
    "src_input_ids = tokenized_output[\"input_ids\"] # [batch, tokenizer]\n",
    "src_attention_mask = tokenized_output[\"attention_mask\"]\n",
    "\n",
    "print(src_input_ids.shape)\n",
    "print(src_attention_mask.shape)\n",
    "\n",
    "# # step2 E & PE\n",
    "# embedding_layer = nn.Embedding(\n",
    "#     num_embeddings=tokenizer.vocab_size,  # 词汇表大小\n",
    "#     embedding_dim=embedding_dim          # 嵌入维度\n",
    "# )\n",
    "# position_embedding = nn.Embedding(max_seq_len, embedding_dim)"
   ],
   "id": "1fd2bf3d8affb37c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T13:34:35.418295Z",
     "start_time": "2024-12-05T13:34:34.769724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 编码目标序列\n",
    "encoded_target = tokenizer(target_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "tgt_input_ids = encoded_target['input_ids']\n",
    "tgt_attention_mask = encoded_target['attention_mask']\n",
    "\n",
    "# 准备交叉注意力的 src_mask\n",
    "src_mask = src_attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# 准备自注意力的 tgt_mask\n",
    "seq_len = tgt_input_ids.size(1)\n",
    "tgt_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\n",
    "tgt_mask = tgt_mask.to(tgt_input_ids.device).expand(tgt_input_ids.size(0), num_heads, seq_len, seq_len)\n",
    "\n",
    "# 初始化 Decoder 并获取输出\n",
    "decoder = Decoder(num_layers, d_model, num_heads, d_ff, dropout, tokenizer.vocab_size)\n",
    "output, attn_weights = decoder(tgt_input_ids, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "# 输出的形状应该是 [batch_size, seq_len, vocab_size]\n",
    "print(output.shape)  # 应该是 (32, 10, 10000)"
   ],
   "id": "9542c3a7c97de37",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (160) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[113], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# 初始化 Decoder 并获取输出\u001B[39;00m\n\u001B[0;32m     15\u001B[0m decoder \u001B[38;5;241m=\u001B[39m Decoder(num_layers, d_model, num_heads, d_ff, dropout, tokenizer\u001B[38;5;241m.\u001B[39mvocab_size)\n\u001B[1;32m---> 16\u001B[0m output, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt_input_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# 输出的形状应该是 [batch_size, seq_len, vocab_size]\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(output\u001B[38;5;241m.\u001B[39mshape)  \u001B[38;5;66;03m# 应该是 (32, 10, 10000)\u001B[39;00m\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[88], line 30\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[1;34m(self, x, enc_output, src_mask, tgt_mask)\u001B[0m\n\u001B[0;32m     27\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, dec_layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdec_layers):\n\u001B[1;32m---> 30\u001B[0m     x, block1, block2 \u001B[38;5;241m=\u001B[39m \u001B[43mdec_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m     attention_weights[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdecoder_layer\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_block1\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m block1\n\u001B[0;32m     32\u001B[0m     attention_weights[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdecoder_layer\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_block2\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m block2\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[87], line 45\u001B[0m, in \u001B[0;36mDecoderLayer.forward\u001B[1;34m(self, x, enc_output, src_mask, tgt_mask)\u001B[0m\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(attn_output))  \u001B[38;5;66;03m# 残差连接和 LayerNorm\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# 交叉注意力：输入为编码器输出（enc_output）和解码器输入（x）\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m attn_output, block2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(attn_output))  \u001B[38;5;66;03m# 残差连接和 LayerNorm\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# 前馈网络（Feed-Forward）\u001B[39;00m\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\BaiduNetdiskDownload\\Software\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[81], line 45\u001B[0m, in \u001B[0;36mMultiHeadAttention.forward\u001B[1;34m(self, q, k, v, mask)\u001B[0m\n\u001B[0;32m     42\u001B[0m scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(q, k\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale  \u001B[38;5;66;03m# (batch_size, num_heads, seq_len_q, seq_len_k)\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 45\u001B[0m     scores \u001B[38;5;241m=\u001B[39m \u001B[43mscores\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmasked_fill\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1e9\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m attention_weights \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(scores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# (batch_size, num_heads, seq_len_q, seq_len_k)\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m# 应用注意力权重到值向量\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (4) must match the size of tensor b (160) at non-singleton dimension 3"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:13:18.579677Z",
     "start_time": "2024-12-05T12:13:18.574137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 生成嵌入\n",
    "token_embeddings = embedding_layer(input_ids) # [batch_size, seq_len, emb]\n",
    "print(token_embeddings.shape)\n",
    "# Position Embedding\n",
    "batch_size = token_embeddings.shape[0]\n",
    "seq_len = token_embeddings.shape[1]\n",
    "\n",
    "positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, seq_len) # [batch_size, seq_len]\n",
    "print(positions.shape)\n",
    "position_embeddings = position_embedding(positions)  # [batch_size, seq_len, embed_dim] \n",
    "print(position_embeddings.shape)\n",
    "# Combine Token and Position Embedding\n",
    "# this is the final embedding inputting to decoder\n",
    "\n",
    "embeddings = token_embeddings + position_embeddings  # [batch_size, seq_len, embed_dim]\n",
    "\n",
    "# Prepare Causal Mask for Decoder 对于生成任务，还会添加 causal mask（自回归掩码），确保当前 token 只关注自身及之前的 token。\n",
    "# this is the final mask inputting to decoder\n",
    "# final_mask 的维度应该是多大的？？？？？？？？？？？？？？？？？？？？？？ 生成的掩码的维度和embedding后的维度的关系\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1).bool()\n",
    "final_mask = causal_mask | ~attention_mask.bool().unsqueeze(1).expand(-1, seq_len, -1)  # Combine causal and attention masks\n",
    "print(causal_mask.shape)\n",
    "print(final_mask.shape)\n",
    "# step2 final answer:embedding & mask\n",
    "\n",
    "# 查看分词结果\n",
    "# print(\"Input Text:\", input_text)\n",
    "# print(\"Tokenized IDs:\", tokenized_output[\"input_ids\"])\n",
    "# print(\"Attention Mask:\", tokenized_output[\"attention_mask\"])\n",
    "# print(tokenized_output)\n",
    "# print(len(tokenized_output[\"input_ids\"][0]))\n",
    "# print(\"Embeddings Shape:\", embeddings.shape)\n",
    "# print(\"tokenizer.vocab_size:\", tokenizer.vocab_size)"
   ],
   "id": "72f23156516d4e86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 512])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1, 8, 512])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([1, 8, 8])\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![连接](./decoder_img/emb_to_mha.png)",
   "id": "b5d5a06e993d922a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. how to use embedding to generate the final answer?\n",
    "## embedding + final_mask\n",
    "\n",
    "### 构造 Q K V"
   ],
   "id": "c5d5bea7649e2213"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### my purpose\n",
    "\n",
    "# load tokenizer and decoder/encoder/encoder-decoder model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenizer\n",
    "inputs = tokenizer(\"Hello, my dog is cute.\", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "# use decoder to generate\n",
    "generated_ids = model.generate(**inputs)\n",
    "# decoder to text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)\n",
    "\n",
    "\n",
    "# use encoder to embedding\n",
    "outputs = model(**inputs)\n",
    "# get the last layer's hidden state\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(\"Embeddings shape:\", last_hidden_states.shape)"
   ],
   "id": "11beea32512994a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
