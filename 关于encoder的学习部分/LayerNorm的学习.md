# **需要的素材**

`LayerNorm` 是一种归一化方法，用于对输入特征按层进行归一化，确保输出具有零均值和单位方差。

`torch.nn.LayerNorm` 是 PyTorch 中提供的标准实现，适用于**对指定的特征维度**进行归一化。

# **Add&Norm层的作用**

残差连接（Add）：解决梯度消失问题，稳定训练。当网络很深时，梯度会随着层数加深逐渐减小，导致前面的层无法更新。通过残差连接，梯度可以绕过子层直接传播，**缓解梯度消失问题，使得训练更加稳定**。同时，残差连接允许子层的输入直接参与后续计算，确保**即使子层的学习效果不佳，模型仍然能通过输入的信息流获得基本的性能**。

归一化（Norm）：加速收敛，提高数值稳定性。子层输出的特征分布可能随训练阶段不断变化，LayerNorm 通过归一化操作**让每层的输入特征分布稳定**，加速模型收敛。同时，确保残差连接后的数值范围在较为稳定的区间内（均值为 0，方差为 1），**防止过大或过小的数值影响后续计算**。

# **LayerNorm 的内部实现流程**

1. **计算均值和标准差：**
    对指定的特征维度计算输入张量的均值和标准差。

2. **归一化处理：**
    使用以下公式将输入值归一化：

   $\text{Norm}(x) = \frac{x - \mu}{\sigma + \epsilon}$

   其中：

   - μ是均值
   - σ 是标准差
   - ϵ 是一个小值，用于避免分母为零

3. **添加可学习的参数：**
    通过可学习的缩放参数 (γ) 和偏移参数 (β) 调整归一化后的值：

   Output=γ⋅Norm(x)+β



#### **1. 初始化 `LayerNorm`**

`torch.nn.LayerNorm` 会初始化两个重要参数：

- `normalized_shape`：指定归一化的维度
- `eps`：避免除零的小值（默认值是 1e−51e^{-5}）

```python
import torch
import torch.nn as nn

# 假设输入是 3 个特征，每个特征维度大小为 4
normalized_shape = [4]

# 初始化 LayerNorm
layer_norm = nn.LayerNorm(normalized_shape, eps=1e-5)
```

在这里，`normalized_shape` 为 `[4]`，表示每个特征维度（最后一个维度）会进行归一化处理。

------

#### **2. 输入张量**

输入张量 `x`：

```python
x = torch.tensor([[1.0, 2.0, 3.0, 4.0], 
                  [5.0, 6.0, 7.0, 8.0]])
```

#### **3. 计算均值和标准差**

对每一行的最后一个维度进行归一化：

- **均值 (μ\mu)**： 对于第一行，均值是：

  $\mu = \frac{1+2+3+4}{4} = 2.5$

  对于第二行，均值是：

  $\mu = \frac{5+6+7+8}{4} = 6.5$

- **标准差 (σ\sigma)**： 对第一行：

  $\sigma = \sqrt{\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4}} = 1.118$

  对第二行：

  $\sigma = \sqrt{\frac{(5-6.5)^2 + (6-6.5)^2 + (7-6.5)^2 + (8-6.5)^2}{4}} = 1.118$

#### **4. 归一化处理**

归一化公式：

$\text{Norm}(x) = \frac{x - \mu}{\sigma + \epsilon}$

对第一行：

$\text{Norm}([1, 2, 3, 4]) = [\frac{1-2.5}{1.118}, \frac{2-2.5}{1.118}, \frac{3-2.5}{1.118}, \frac{4-2.5}{1.118}] = [-1.34, -0.45, 0.45, 1.34]$

对第二行：

$\text{Norm}([5, 6, 7, 8]) = [-1.34, -0.45, 0.45, 1.34]$

#### **5. 添加缩放和偏移**

$\text{Output} = \gamma \cdot \text{Norm}(x) + \beta$

假设 γ=1，β=0，则输出结果与归一化值相同：

```python
output = layer_norm(x)
```

------

#### **完整代码实现**

```python
import torch
import torch.nn as nn

# 初始化 LayerNorm
normalized_shape = [4]
layer_norm = nn.LayerNorm(normalized_shape, eps=1e-5)

# 输入张量
x = torch.tensor([[1.0, 2.0, 3.0, 4.0], 
                  [5.0, 6.0, 7.0, 8.0]])

# 执行 LayerNorm
output = layer_norm(x)

# 打印输出
print("归一化输出：\n", output)
```

------

#### **输出解释**

归一化后的张量：

```text
归一化输出：
tensor([[-1.34, -0.45, 0.45, 1.34],
        [-1.34, -0.45, 0.45, 1.34]])
```

通过 `LayerNorm`，可以有效去除特征维度的均值和方差差异，同时保持整个层的特性不变。

# FAQ

## **Q：为什么用LayerNorm不用BatchNorm1d层？**

两者的主要区别在于：

1. **归一化维度**：

   **BatchNorm**：对批次维度（batch）内的统计量（均值和方差）进行归一化，适用于卷积网络等场景；**LayerNorm**：对每个样本的特征维度进行归一化，适用于 NLP 和序列建模等场景。

2. **依赖的统计量**：

   **BatchNorm**：使用整个批次内的均值和方差进行归一化；**LayerNorm**：使用每个样本的特征维度的均值和方差进行归一化。

假设：批次大小为 2，每个样本有 3 个特征，每个特征有 4 个数值

```python
x = torch.tensor([
    [[1.0, 2.0, 3.0, 4.0],  # 样本 1，第 1 个特征
     [5.0, 6.0, 7.0, 8.0],  # 样本 1，第 2 个特征
     [9.0, 10.0, 11.0, 12.0]],  # 样本 1，第 3 个特征

    [[13.0, 14.0, 15.0, 16.0],  # 样本 2，第 1 个特征
     [17.0, 18.0, 19.0, 20.0],  # 样本 2，第 2 个特征
     [21.0, 22.0, 23.0, 24.0]]  # 样本 2，第 3 个特征
])
```

<u>对于 **BatchNorm**，归一化基于批次内每个特征位置的均值和方差。</u>

1. **统计量计算**
    对于第一个特征（所有样本的第一个特征位置）：

   $\mu = \frac{1+13}{2} = 7, \quad \sigma^2 = \frac{(1-7)^2 + (13-7)^2}{2} = 36$

   对于第二个特征：

   $\mu = \frac{2+14}{2} = 8, \quad \sigma^2 = \frac{(2-8)^2 + (14-8)^2}{2} = 36$

   同理，对所有特征位置计算均值和方差。

2. **归一化** 按特征位置归一化：

   $\text{Norm}(x_{i,d}) = \frac{x_{i,d} - \mu_d}{\sqrt{\sigma_d^2 + \epsilon}}$

   比如，第一个特征位置：

   $\text{Norm}(1) = \frac{1 - 7}{\sqrt{36 + \epsilon}} = -1.0, \quad \text{Norm}(13) = \frac{13 - 7}{\sqrt{36 + \epsilon}} = 1.0$



<u>对于 **LayerNorm**，归一化基于每个样本独立的特征维度。</u>

1. **统计量计算**
    对于第一个样本：

   $\mu = \frac{1+2+3+4+5+6+7+8+9+10+11+12}{12} = 6.5, \quad \sigma^2 = \text{Var}([1, 2, ..., 12])$

   对第二个样本：

   $\mu = \frac{13+14+...+24}{12} = 18.5, \quad \sigma^2 = \text{Var}([13, 14, ..., 24])$

2. **归一化** 按每个样本的特征归一化：

   $\text{Norm}(x_{i,j}) = \frac{x_{i,j} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}$

   比如，对于样本 1：

   $\text{Norm}(1) = \frac{1 - 6.5}{\sqrt{\sigma^2 + \epsilon}}$

#### 

以下是用 PyTorch 分别计算 BatchNorm 和 LayerNorm 的示例：

```python
import torch
import torch.nn as nn

# 输入张量
x = torch.tensor([
    [[1.0, 2.0, 3.0, 4.0],
     [5.0, 6.0, 7.0, 8.0],
     [9.0, 10.0, 11.0, 12.0]],
    [[13.0, 14.0, 15.0, 16.0],
     [17.0, 18.0, 19.0, 20.0],
     [21.0, 22.0, 23.0, 24.0]]
])

# BatchNorm，假设特征维度为 4
batch_norm = nn.BatchNorm1d(num_features=4)
x_bn = x.permute(0, 2, 1)  # 将维度调整为 (batch, features, seq)因为BatchNorm1d默认第一列是特征
output_bn = batch_norm(x_bn)
x_original = output_bn .permute(0, 2, 1)
print("BatchNorm 输出：\n", output_bn)

# LayerNorm
layer_norm = nn.LayerNorm(normalized_shape=(4,))
output_ln = layer_norm(x)
print("LayerNorm 输出：\n", output_ln)
```

两者的输出会有所不同，BatchNorm 强调批次特征的一致性，而 LayerNorm 更关注单样本的归一化。

## **Q: 为什么LayNorm层要指定参数？**

输入数据的形状可能有多种格式，不同维度代表的含义可能不同，LayerNorm 无法在这些场景中自动推断正确的归一化维度。例如：

- **场景 1：NLP 输入**
  输入形状 (B,T,D)，通常归一化最后一维（特征维度 D）。
- **场景 2：图像输入**
  输入形状 (B,C,H,W)，可能需要对某些特定维度（例如通道维度 C）归一化。

举例：

**NLP输入** 输入形状：(B,T,D)

B：批次大小（Batch size）。T：序列长度（Sequence length），例如一个句子的单词数。D：特征维度（Feature dimension），例如嵌入向量的维度。

```python
x = torch.tensor([
    [[1.0, 2.0, 3.0],  # 句子 1，第 1 个单词的特征向量
     [4.0, 5.0, 6.0],  # 句子 1，第 2 个单词的特征向量
     [7.0, 8.0, 9.0]], # 句子 1，第 3 个单词的特征向量

    [[10.0, 11.0, 12.0],  # 句子 2，第 1 个单词的特征向量
     [13.0, 14.0, 15.0],  # 句子 2，第 2 个单词的特征向量
     [16.0, 17.0, 18.0]]  # 句子 2，第 3 个单词的特征向量
])  # 形状 (2, 3, 3)
```

**归一化维度：**
在 NLP 中，通常对最后一维（特征维度 D）进行归一化，因为每个时间步的特征向量独立，需要归一化它的值。

```python
layer_norm = nn.LayerNorm(normalized_shape=(3,))  # 特征维度为 3
output = layer_norm(x)
```



**图像输入** （B，C，H，W）

B：批次大小（Batch size）。C：通道数（Channels），例如 RGB 图像的通道数为 3。H：高度（Height）。W：宽度（Width）。

任务：在图像分类任务中，LayerNorm 可以对每个图像的通道维度进行归一化。

```python
x = torch.tensor([
    [[[1.0, 2.0], [3.0, 4.0]],  # 通道 1
     [[5.0, 6.0], [7.0, 8.0]],  # 通道 2
     [[9.0, 10.0], [11.0, 12.0]]],  # 通道 3

    [[[13.0, 14.0], [15.0, 16.0]],  # 通道 1
     [[17.0, 18.0], [19.0, 20.0]],  # 通道 2
     [[21.0, 22.0], [23.0, 24.0]]]  # 通道 3
])  # 形状 (2, 3, 2, 2)
```

**归一化维度：**
在图像任务中，可能希望对每个通道独立归一化，即每个通道的像素值归一化，因此归一化的维度是 (H,W)(H, W)(H,W)。

```python
layer_norm = nn.LayerNorm(normalized_shape=(2, 2))  # 归一化每个通道的 (H, W)
output = layer_norm(x)
```