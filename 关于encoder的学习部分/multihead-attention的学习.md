









# 流程

![img](https://i-blog.csdnimg.cn/blog_migrate/6bff0da15c478eef4b01e8180e2c601e.png)

# 前情提要

## 理解如何拆除multihead

我们以一个具体的输入张量为例，详细展示 `split_heads` 操作如何改变维度，以及为什么将 `num_heads` 放到第 2 维有助于矩阵操作。

```python
import torch
import math
batch_size = 2
seq_length = 4
d_model = 8
num_heads = 2
d_k = d_model // num_heads
# 定义输入张量
x = torch.tensor([
    [
        [1, 2, 3, 4, 5, 6, 7, 8],
        [9, 10, 11, 12, 13, 14, 15, 16],
        [17, 18, 19, 20, 21, 22, 23, 24],
        [25, 26, 27, 28, 29, 30, 31, 32]
    ],
    [
        [33, 34, 35, 36, 37, 38, 39, 40],
        [41, 42, 43, 44, 45, 46, 47, 48],
        [49, 50, 51, 52, 53, 54, 55, 56],
        [57, 58, 59, 60, 61, 62, 63, 64]
    ]
])  # 形状 [2, 4, 8]

# 重塑张量
x = x.view(batch_size, seq_length, num_heads, d_k)  # [2, 4, 2, 4]
print(x)
# 调整维度顺序
y = x.permute(0, 2, 1, 3)  # [2, 2, 4, 4]
# 打印新张量的形状
print(y)

z = x.transpose(1,2)
print(z)
```

输出

```python
#x如下
tensor([[[[ 1,  2,  3,  4],
          [ 5,  6,  7,  8]],

         [[ 9, 10, 11, 12],
          [13, 14, 15, 16]],

         [[17, 18, 19, 20],
          [21, 22, 23, 24]],

         [[25, 26, 27, 28],
          [29, 30, 31, 32]]],


        [[[33, 34, 35, 36],
          [37, 38, 39, 40]],

         [[41, 42, 43, 44],
          [45, 46, 47, 48]],

         [[49, 50, 51, 52],
          [53, 54, 55, 56]],

         [[57, 58, 59, 60],
          [61, 62, 63, 64]]]])
          
#y如下
tensor([[[[ 1,  2,  3,  4],
          [ 9, 10, 11, 12],
          [17, 18, 19, 20],
          [25, 26, 27, 28]],

         [[ 5,  6,  7,  8],
          [13, 14, 15, 16],
          [21, 22, 23, 24],
          [29, 30, 31, 32]]],


        [[[33, 34, 35, 36],
          [41, 42, 43, 44],
          [49, 50, 51, 52],
          [57, 58, 59, 60]],

         [[37, 38, 39, 40],
          [45, 46, 47, 48],
          [53, 54, 55, 56],
          [61, 62, 63, 64]]]])
          
#z如下
tensor([[[[ 1,  2,  3,  4],
          [ 9, 10, 11, 12],
          [17, 18, 19, 20],
          [25, 26, 27, 28]],

         [[ 5,  6,  7,  8],
          [13, 14, 15, 16],
          [21, 22, 23, 24],
          [29, 30, 31, 32]]],


        [[[33, 34, 35, 36],
          [41, 42, 43, 44],
          [49, 50, 51, 52],
          [57, 58, 59, 60]],

         [[37, 38, 39, 40],
          [45, 46, 47, 48],
          [53, 54, 55, 56],
          [61, 62, 63, 64]]]])
```



# 实现流程

输入是[batch_size, seq_length, d_model]的格式，因为要计算注意力，所以要分成三份，放到多头注意力的输入中。

在`__init__`中设置给QKV的权重，之后输入进来的三个x先乘权重，之后分成多头的样子；并把多头的数量放在（从一开始数的）第二列【因为最后两列会涉及矩阵的乘法，所以把多头的数量放前面】

然后开始相乘attention，K转置，实现公式

![image-20241126214534453](C:\Users\16864\AppData\Roaming\Typora\typora-user-images\image-20241126214534453.png)

最后要把之前分出来的头在合回去，过程是把(batch_size, self.num_heads, seq_length, self.d_k)转为(batch_size, seq_length, self.num_heads,  self.d_k)，然后再变为(batch_size, seq_length, self.d_model)





# FAQ

**为什么要多头？**

- **固定的投影方式**：单头注意力只用一个 $W_Q, W_K, W_V$集合对输入进行特征投影。这个投影只能捕获一种视角下的特征分布。
- **无法解耦特征**：所有信息都集中在一个高维空间中，难以解耦不同的依赖关系，比如同时捕获局部和全局信息。

多头注意力并不是简单地将数字“拆分成小组”，而是让每个头：

1. 独立初始化权重：
   - 每个头有自己的 $W_Q, W_K, W_V$。
   - 因此，每个头对输入的查询（query）、键（key）、和值（value）进行投影的方式不同。
2. 独立优化目标：
   - 每个头计算自己的注意力分布 $\text{softmax}(QK^T / \sqrt{d_k})$，并独立学习如何关注特定的信息。

这种分割方式是<u>维度分割</u>，而不是直接分割。按特征维度分割，每个头关注低维特征子空间，每个头都能看到完整序列，全局上下文保留，每个头在低维特征空间中独立优化，形成分工。

相关技术有

<u>动态调整每个头的维度，而不是简单地均匀分割：</u>

​	**Dynamic Head Allocation**：在训练中动态调整每个头的特征维度。

​	**Learnable Head Dimensions**：让模型自动学习每个头应分配的维度。

<u>Lazy head问题：</u>

​	**LayerDrop**：在训练中随机丢弃部分头，增强剩余头的鲁棒性。

​	**Attention Head Importance Analysis**：计算头对最终损失的贡献，剪枝低重要性头。

<u>增强头之间的信息共享：</u>

​	**Head Clustering**：将功能相近的头聚合，使其协同工作。

结合稀疏和全局注意力机制，使部分头关注局部窗口，部分头关注全局上下文：

​	**Sparse Transformer**：在局部注意力和全局注意力之间混合。



**为什么要用$\sqrt(d_k)$缩放？**

$QK^{⊤}$的点积数值可能过大

- 对于随机初始化的 Q 和 K，每个分量的方差通常是 $\frac{1}{d_k}$（假设标准初始化方法）。
- 因此，点积的结果期望值是 $ d_k\cdot \frac{1}{d_k} = 1$，但它的方差是 $d_k $倍的放大，即：

$\text{Var}(Q_i \cdot K_j) = d_k \cdot \text{Var}(q_k) \cdot \text{Var}(k_k) \approx d_k \cdot \frac{1}{d_k^2} = \frac{1}{d_k}.$

- 当 $d_k $较大时，点积结果会变得很大，这会导致 $\text{softmax} $的输入值过大，使得其梯度变得非常小，进而影响训练效率。

举例：

当$d_k=4$时，假设q=[0.5,−0.8,1.2,−0.4]，k=[1.0,−0.5,0.7,−0.3]

点积结果为：

$q \cdot k = (0.5 \cdot 1.0) + (-0.8 \cdot -0.5) + (1.2 \cdot 0.7) + (-0.4 \cdot -0.3) = 0.5 + 0.4 + 0.84 + 0.12 = 1.86$

当$d_k=64$时，假设 q和k的每个元素依然随机采样自 N(0,1)，计算的点积结果将是 64 个独立随机变量的和。由于每个随机变量的方差是 1，总和的方差是 64，结果的标准差是8。这意味着点积结果的数值范围会显著扩大，通常接近于 ±8。

所以$\sqrt{d_k}$ 缩放后，$\text{softmax}$ 的输入分布范围更稳定，模型可以更快地收敛。



- **Q: 为什么用transpose来调换维度而不是用permute？**

因为在多头注意力中，我们通常只需要交换两个维度（`seq_length` 和 `num_heads`），`transpose` 修改的元数据更少，性能开销比 `permute` 更低，在底层实现上更直接，更高效、更简洁。permute 是更通用的维度重新排列工具，适用于复杂场景，例如从[a,b,c,d] 到[d,b,a,c]。

不过permute和transpose输出后的张量内容也不一样

- **Q: 为什么用view来改变维度而不是用reshape？**

`view()` 返回一个新的张量，与原始张量共享相同的数据内存，不会发生数据复制，因此操作更加高效。使用 `view()` 的前提是张量在内存中是连续的。如果张量不是连续的，需要先调用 `contiguous()` 方法将其转换为连续的。

`reshape()` 可以处理非连续的张量。如果数据不连续，`reshape()` 可能会返回一个数据副本，这涉及到数据复制，可能会影响性能，由于可能发生数据复制，`reshape()` 的性能可能不如 `view()` 高效。

虽然在非连续张量上先调用 `contiguous()` 然后使用 `view()` 可以达到与 `reshape()` 相同的效果，但这可能会导致不必要的数据复制，影响性能。因此，在处理非连续张量时，直接使用 `reshape()` 更为合适。

**Q: 为什么经过两次 `transpose` 后张量已经恢复原样，明明应该（可能）是连续的，为什么使用 `view` 还需要调用 `contiguous()`？**

在 `forward` 方法中，计算 `attn_output = self.dot_attention(Q, K, V, mask)` 之后，`dot_attention` 的输出，涉及矩阵乘法和 softmax 等操作。这些操作可能会生成内存中**非连续**的张量。因此，对这样的张量进行转置会得到一个<u>非连续的张量。</u>

于是这时用reshape可能会更高效